{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN.ipyb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNP8yNLWIIBim007at4O5W4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HERIUN/RLstudy/blob/master/DQN_ipyb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOXG4Anpch77",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import collections\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkx65H9-cpLX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.0005\n",
        "gamma         = 0.98\n",
        "buffer_limit  = 50000\n",
        "batch_size    = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlvXfJfodJI-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayBuffer():\n",
        "  def __init__(self):\n",
        "    self.buffer = collections.deque(maxlen=buffer_limit)\n",
        "  \n",
        "  def put(self,experience): #experience = obs, a, reward, next_obs, done_mask\n",
        "    self.buffer.append(experience)\n",
        "\n",
        "  def sample(self, n):\n",
        "    mini_batch = random.sample(self.buffer, n)\n",
        "    obs_lst, a_lst, r_lst, next_obs_lst, done_mask_lst = [], [], [], [], []\n",
        "        \n",
        "    for experience in mini_batch:\n",
        "      obs, a, r, next_obs, done_mask = experience\n",
        "      obs_lst.append(obs)\n",
        "      a_lst.append([a])\n",
        "      r_lst.append([r])\n",
        "      next_obs_lst.append(next_obs)\n",
        "      done_mask_lst.append([done_mask])\n",
        "\n",
        "      return torch.tensor(obs_lst, dtype=torch.float), torch.tensor(a_lst), \\\n",
        "             torch.tensor(r_lst), torch.tensor(next_obs_lst, dtype=torch.float), \\\n",
        "             torch.tensor(done_mask_lst)\n",
        "\n",
        "  \n",
        "  def size(self):\n",
        "    return len(self.buffer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtXHgHhTeJCd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Qnet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Qnet, self).__init__()\n",
        "    self.fc1 = nn.Linear(4, 128) ##  4의 의미는 obs, action, reward, next_obs. 4개라서\n",
        "    self.fc2 = nn.Linear(128, 128)\n",
        "    self.fc3 = nn.Linear(128, 2) ## 2의 의미는 action의 가지수\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x\n",
        "  \n",
        "  def sample_action(self, obs, epsilon):  ## e-greedy 구현\n",
        "    out = self.forward(obs)\n",
        "    coin = random.random()   # 0~1까지의 실수 \n",
        "    if coin < epsilon:\n",
        "      return random.randint(0,1)  # action을 0(왼)또는 1(오른)random하게 해라!\n",
        "    else:\n",
        "      return out.argmax().item()  # obs -> nn -> output action중 큰걸 해라!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqCIzfIUDHe1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "  env = gym.make('CartPole-v1')\n",
        "  q = Qnet()        #예측\n",
        "  q_target = Qnet() #실제 닮아야할 값\n",
        "  q_target.load_state_dict(q.state_dict()) #일단 q_target의 wetigh를 q의 weight로 복사\n",
        "  exp_memory = ReplayBuffer()\n",
        "  score = 0.0\n",
        "\n",
        "  optimizer = optim.Adam(q.parameters(), lr=learning_rate) \n",
        "  #q를 gradeint dscent로 파라미터 업데이트 하는거임 not q_target\n",
        "  \n",
        "  for n_epi in range(10000):\n",
        "    epsilon = 0.1 # decaying e-greedy 10% -> 1%\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "      a = q.sample_action(torch.from_numpy(obs).float(), epsilon)\n",
        "      next_obs, reward, done, info = env.step(a) #info는 디버깅할때 쓸 정보\n",
        "      # env.step(env.action_space.sample()) = take random action\n",
        "      done_mask = 0.0 if done else 1.0 #마지막 state의 qvalue는 0으로 하려고 만듬\n",
        "      exp_memory.put((obs,a,reward/100.0,next_obs,done_mask))\n",
        "      obs = next_obs\n",
        "\n",
        "      score +=reward\n",
        "\n",
        "      if done:\n",
        "        break\n",
        "    \n",
        "    if exp_memory.size() > 2000: #2000개 이상 exp가 있을때부터 학습 시작하자\n",
        "      train(q,q_target,exp_memory,optimizer)\n",
        "\n",
        "    if n_epi%20==0 and n_epi!=0:\n",
        "      q_target.load_state_dict(q.state_dict()) #20번 에피소드마다 target network를 업데이트\n",
        "      print(\"n_episode : {}, score : {:.1f}, n_buffer : {}, eps : {:.1f}%\".format(\n",
        "             n_epi, score/20, exp_memory.size(), epsilon*100))\n",
        "      \n",
        "      score = 0.0 # score는 버틴 시간수치\n",
        "  env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehN1PkOVRZYb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(q, q_target, exp_memory, optimizer):\n",
        "  for i in range(10):\n",
        "    obs,a,r,next_obs,done_mask = exp_memory.sample(batch_size)\n",
        "    #episode가 한번 끝날때마다 샘플 32개로 10번 총 320개 샘플로 weight가 업데이트됨\n",
        "\n",
        "    q_out = q(obs) #s's shape[32,4] q(s)'s shape[32,2]\n",
        "    q_a = q_out.gather(1,a) #취한 action의 q값만 골라냄 [32,1]\n",
        "    max_q_prime = q_target(next_obs).max(1)[0].unsqueeze(1)\n",
        "    # q_target의 shape [32,2]에서 max 취하면 [32] 거기서 unsqueeze하면 [32,1]\n",
        "    target = r + gamma * max_q_prime * done_mask\n",
        "    # done_mask[32,1]는 마지막 state일시에 0을 곱하는 용도\n",
        "    loss = F.smooth_l1_loss(target, q_a)\n",
        "\n",
        "    optimizer.zero_grad() #optimizer의 gradient를 0으로 비우고\n",
        "    loss.backward() #gard가 backprop되면서 구해지고\n",
        "    optimizer.step() # 그 grad를 이용해 weight들이 업데이트 됨"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ck65XEV3UUj6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUNPi3IlWdp5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}