{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RLlib Tutorial",
      "provenance": []
    },
    "hide_code_all_hidden": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7ghlHAH0Nlhi"
      },
      "source": [
        "# Install Dependencies\n",
        "\n",
        "If you are running on Google Colab, you need to install the necessary dependencies before beginning the exercise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tHNPcnXPNlhj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 897
        },
        "outputId": "ec45d7b6-3022-4ad6-800d-fbbb23dd7571"
      },
      "source": [
        "print('NOTE: Intentionally crashing session to use the newly installed library.\\n')\n",
        "\n",
        "!pip uninstall -y pyarrow\n",
        "!pip install ray[debug]==0.7.5\n",
        "!pip install bs4\n",
        "\n",
        "!git clone https://github.com/ray-project/tutorial || true\n",
        "from tutorial.rllib_exercises import test_exercises\n",
        "\n",
        "print(\"Successfully installed all the dependencies!\")\n",
        "\n",
        "# A hack to force the runtime to restart, needed to include the above dependencies.\n",
        "import os\n",
        "os._exit(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NOTE: Intentionally crashing session to use the newly installed library.\n",
            "\n",
            "Uninstalling pyarrow-0.14.1:\n",
            "  Successfully uninstalled pyarrow-0.14.1\n",
            "Collecting ray[debug]==0.7.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/e7/37a7f8dc2b1f96c760a3950d8bb5a3f14066f1699f5d004f0f6462d880c9/ray-0.7.5-cp36-cp36m-manylinux1_x86_64.whl (74.9MB)\n",
            "\u001b[K     |████████████████████████████████| 74.9MB 115kB/s \n",
            "\u001b[?25hRequirement already satisfied: jsonschema in /usr/local/lib/python3.6/dist-packages (from ray[debug]==0.7.5) (2.6.0)\n",
            "Collecting redis>=3.3.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a7/7c/24fb0511df653cf1a5d938d8f5d19802a88cef255706fdda242ff97e91b7/redis-3.5.3-py2.py3-none-any.whl (72kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 8.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from ray[debug]==0.7.5) (3.13)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from ray[debug]==0.7.5) (7.1.2)\n",
            "Requirement already satisfied: six>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from ray[debug]==0.7.5) (1.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from ray[debug]==0.7.5) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.6/dist-packages (from ray[debug]==0.7.5) (1.18.5)\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/c9/dc/45cdef1b4d119eb96316b3117e6d5708a08029992b2fee2c143c7a0a5cc5/colorama-0.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from ray[debug]==0.7.5) (3.6.4)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from ray[debug]==0.7.5) (3.12.4)\n",
            "Collecting funcsigs\n",
            "  Downloading https://files.pythonhosted.org/packages/69/cb/f5be453359271714c01b9bd06126eaf2e368f1fddfff30818754b5ac2328/funcsigs-1.0.2-py2.py3-none-any.whl\n",
            "Collecting setproctitle; extra == \"debug\"\n",
            "  Downloading https://files.pythonhosted.org/packages/5a/0d/dc0d2234aacba6cf1a729964383e3452c52096dc695581248b548786f2b3/setproctitle-1.1.10.tar.gz\n",
            "Requirement already satisfied: psutil; extra == \"debug\" in /usr/local/lib/python3.6/dist-packages (from ray[debug]==0.7.5) (5.4.8)\n",
            "Collecting py-spy; extra == \"debug\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/a7/ab45c9ee3c4654edda3efbd6b8e2fa4962226718a7e3e3be6e3926bf3617/py_spy-0.3.3-py2.py3-none-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 42.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->ray[debug]==0.7.5) (8.5.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->ray[debug]==0.7.5) (0.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->ray[debug]==0.7.5) (1.9.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->ray[debug]==0.7.5) (50.3.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->ray[debug]==0.7.5) (20.2.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->ray[debug]==0.7.5) (1.4.0)\n",
            "Building wheels for collected packages: setproctitle\n",
            "  Building wheel for setproctitle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for setproctitle: filename=setproctitle-1.1.10-cp36-cp36m-linux_x86_64.whl size=33925 sha256=e9a06cba4eab4d594e668c291ea27715faee33f06e227e3768e7a10a30b57491\n",
            "  Stored in directory: /root/.cache/pip/wheels/e6/b1/a6/9719530228e258eba904501fef99d5d85c80d52bd8f14438a3\n",
            "Successfully built setproctitle\n",
            "Installing collected packages: redis, colorama, funcsigs, setproctitle, py-spy, ray\n",
            "Successfully installed colorama-0.4.3 funcsigs-1.0.2 py-spy-0.3.3 ray-0.7.5 redis-3.5.3 setproctitle-1.1.10\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.6/dist-packages (0.0.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from bs4) (4.6.3)\n",
            "Cloning into 'tutorial'...\n",
            "remote: Enumerating objects: 33, done.\u001b[K\n",
            "remote: Counting objects: 100% (33/33), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 952 (delta 14), reused 12 (delta 8), pack-reused 919\u001b[K\n",
            "Receiving objects: 100% (952/952), 31.16 MiB | 28.69 MiB/s, done.\n",
            "Resolving deltas: 100% (525/525), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "hideCode": false,
        "hidePrompt": false,
        "id": "bAptEafKNlhm"
      },
      "source": [
        "# RL Exercise - Markov Decision Processes\n",
        "\n",
        "**GOAL:** The goal of the exercise is to introduce the Markov Decision Process abstraction and to show how to use Markov Decision Processes in Python.\n",
        "\n",
        "**The key abstraction in reinforcement learning is the Markov decision process (MDP).** An MDP models sequential interactions with an external environment. It consists of the following:\n",
        "- a **state space**\n",
        "- a set of **actions**\n",
        "- a **transition function** which describes the probability of being in a state $s'$ at time $t+1$ given that the MDP was in state $s$ at time $t$ and action $a$ was taken\n",
        "- a **reward function**, which determines the reward received at time $t$\n",
        "- a **discount factor** $\\gamma$\n",
        "\n",
        "More details are available [here](https://en.wikipedia.org/wiki/Markov_decision_process).\n",
        "\n",
        "**NOTE:** Reinforcement learning algorithms are often applied to problems that don't strictly fit into the MDP framework. In particular, situations in which the state of the environment is not fully observed lead to violations of the MDP assumption. Nevertheless, RL algorithms can be applied anyway.\n",
        "\n",
        "## Policies\n",
        "\n",
        "A **policy** is a function that takes in a **state** and returns an **action**. A policy may be stochastic (i.e., it may sample from a probability distribution) or it can be deterministic.\n",
        "\n",
        "The **goal of reinforcement learning** is to learn a **policy** for maximizing the cumulative reward in an MDP. That is, we wish to find a policy $\\pi$ which solves the following optimization problem\n",
        "\n",
        "\\begin{equation}\n",
        "\\arg\\max_{\\pi} \\sum_{t=1}^T \\gamma^t R_t(\\pi),\n",
        "\\end{equation}\n",
        "\n",
        "where $T$ is the number of steps taken in the MDP (this is a random variable and may depend on $\\pi$) and $R_t$ is the reward received at time $t$ (also a random variable which depends on $\\pi$).\n",
        "\n",
        "A number of algorithms are available for solving reinforcement learning problems. Several of the most widely known are [value iteration](https://en.wikipedia.org/wiki/Markov_decision_process#Value_iteration), [policy iteration](https://en.wikipedia.org/wiki/Markov_decision_process#Policy_iteration), and [Q learning](https://en.wikipedia.org/wiki/Q-learning).\n",
        "\n",
        "## RL in Python\n",
        "\n",
        "The `gym` Python module provides MDP interfaces to a variety of simulators. For example, the CartPole environment interfaces with a simple simulator which simulates the physics of balancing a pole on a cart. The CartPole problem is described at https://gym.openai.com/envs/CartPole-v0. This example fits into the MDP framework as follows.\n",
        "- The **state** consists of the position and velocity of the cart as well as the angle and angular velocity of the pole that is balancing on the cart.\n",
        "- The **actions** are to decrease or increase the cart's velocity by one unit.\n",
        "- The **transition function** is deterministic and is determined by simulating physical laws.\n",
        "- The **reward function** is a constant 1 as long as the pole is upright, and 0 once the pole has fallen over. Therefore, maximizing the reward means balancing the pole for as long as possible.\n",
        "- The **discount factor** in this case can be taken to be 1.\n",
        "\n",
        "More information about the `gym` Python module is available at https://gym.openai.com/."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a9Kwo5ZfNlhn",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import gym\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WPpofaxQNlhp"
      },
      "source": [
        "The code below illustrates how to create and manipulate MDPs in Python. An MDP can be created by calling `gym.make`. Gym environments are identified by names like `CartPole-v0`. A **catalog of built-in environments** can be found at https://gym.openai.com/envs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hideCode": false,
        "hidePrompt": false,
        "id": "6DZ68SG9Nlhp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9904b282-a448-4d70-ac7d-01ce3b5d04b3"
      },
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "print('Created env:', env)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Created env: <TimeLimit<CartPoleEnv<CartPole-v0>>>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Xn5PqgDzNlhr"
      },
      "source": [
        "Reset the state of the MDP by calling `env.reset()`. This call returns the initial state of the MDP."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hideCode": false,
        "hidePrompt": false,
        "id": "zRA58dOFNlhs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "71dd35aa-64ae-439c-a5a1-3c019c5f5d57"
      },
      "source": [
        "state = env.reset()\n",
        "print('The starting state is:', state)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The starting state is: [-0.03475988 -0.01137626  0.00659105  0.00894862]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8MuXXesWNlhu"
      },
      "source": [
        "The `env.step` method takes an action (in the case of the CartPole environment, the appropriate actions are 0 or 1, for moving left or right). It returns a tuple of four things:\n",
        "1. the new state of the environment\n",
        "2. a reward\n",
        "3. a boolean indicating whether the simulation has finished\n",
        "4. a dictionary of miscellaneous extra information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hideCode": false,
        "hidePrompt": false,
        "id": "TufVaMz_Nlhu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4bb17c34-b6c0-46a4-9297-589d1869f46d"
      },
      "source": [
        "# Simulate taking an action in the environment. Appropriate actions for\n",
        "# the CartPole environment are 0 and 1 (for moving left and right).\n",
        "action = 0\n",
        "state, reward, done, info = env.step(action)\n",
        "print(state, reward, done, info)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.0349874  -0.20659212  0.00677002  0.3037038 ] 1.0 False {}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uBIoIuWYNlhw"
      },
      "source": [
        "A **rollout** is a simulation of a policy in an environment. It alternates between choosing actions based (using some policy) and taking those actions in the environment.\n",
        "\n",
        "The code below performs a rollout in a given environment. It takes **random actions** until the simulation has finished and returns the cumulative reward."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Zp00mr88Nlhw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3d3dc928-7930-4a44-9395-de8401d6ecb0"
      },
      "source": [
        "def random_rollout(env):\n",
        "    state = env.reset()\n",
        "    \n",
        "    done = False\n",
        "    cumulative_reward = 0\n",
        "\n",
        "    # Keep looping as long as the simulation has not finished.\n",
        "    while not done:\n",
        "        # Choose a random action (either 0 or 1).\n",
        "        action = np.random.choice([0, 1])\n",
        "        \n",
        "        # Take the action in the environment.\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        \n",
        "        # Update the cumulative reward.\n",
        "        cumulative_reward += reward\n",
        "    \n",
        "    # Return the cumulative reward.\n",
        "    return cumulative_reward\n",
        "    \n",
        "reward = random_rollout(env)\n",
        "print(reward)\n",
        "reward = random_rollout(env)\n",
        "print(reward)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "24.0\n",
            "11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i3FVvEJRNlhy"
      },
      "source": [
        "**EXERCISE:** Finish implementing the `rollout_policy` function below, which should take an environment *and* a policy. The *policy* is a function that takes in a *state* and returns an *action*. The main difference is that instead of choosing a **random action**, the action should be chosen **with the policy** (as a function of the state)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9PgmkROqNlhy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "a10e7896-e851-473b-959b-2cdd164fcab9"
      },
      "source": [
        "def rollout_policy(env, policy):\n",
        "    state = env.reset()\n",
        "    \n",
        "    done = False\n",
        "    cumulative_reward = 0\n",
        "\n",
        "    # Keep looping as long as the simulation has not finished.\n",
        "    while not done:\n",
        "        # Choose a random action (either 0 or 1).\n",
        "        action = policy(state)\n",
        "        \n",
        "        # Take the action in the environment.\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        \n",
        "        # Update the cumulative reward.\n",
        "        cumulative_reward += reward\n",
        "\n",
        "    # Return the cumulative reward.\n",
        "    return cumulative_reward\n",
        "\n",
        "def sample_policy1(state):\n",
        "    return 0 if state[0] < 0 else 1\n",
        "\n",
        "def sample_policy2(state):\n",
        "    return 1 if state[0] < 0 else 0\n",
        "\n",
        "reward1 = np.mean([rollout_policy(env, sample_policy1) for _ in range(100)])\n",
        "reward2 = np.mean([rollout_policy(env, sample_policy2) for _ in range(100)])\n",
        "\n",
        "print('The first sample policy got an average reward of {}.'.format(reward1))\n",
        "print('The second sample policy got an average reward of {}.'.format(reward2))\n",
        "\n",
        "assert 5 < reward1 < 15, ('Make sure that rollout_policy computes the action '\n",
        "                          'by applying the policy to the state.')\n",
        "assert 25 < reward2 < 35, ('Make sure that rollout_policy computes the action '\n",
        "                           'by applying the policy to the state.')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The first sample policy got an average reward of 9.47.\n",
            "The second sample policy got an average reward of 28.15.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qXQ8hIB9Nlh0"
      },
      "source": [
        "# RLlib Exercise 1 - Proximal Policy Optimization\n",
        "\n",
        "**GOAL:** The goal of this exercise is to demonstrate how to use the proximal policy optimization (PPO) algorithm.\n",
        "\n",
        "To understand how to use **RLlib**, see the documentation at http://rllib.io.\n",
        "\n",
        "PPO is described in detail in https://arxiv.org/abs/1707.06347. It is a variant of Trust Region Policy Optimization (TRPO) described in https://arxiv.org/abs/1502.05477\n",
        "\n",
        "PPO works in two phases. In one phase, a large number of rollouts are performed (in parallel). The rollouts are then aggregated on the driver and a surrogate optimization objective is defined based on those rollouts. We then use SGD to find the policy that maximizes that objective with a penalty term for diverging too much from the current policy.\n",
        "\n",
        "![ppo](https://raw.githubusercontent.com/ucbrise/risecamp/risecamp2018/ray/tutorial/rllib_exercises/ppo.png)\n",
        "\n",
        "**NOTE:** The SGD optimization step is best performed in a data-parallel manner over multiple GPUs. This is exposed through the `num_gpus` field of the `config` dictionary (for this to work, you must be using a machine that has GPUs)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d_Luh2OENlh0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5a72e997-f51d-4029-e002-f49e5d6ccc2b"
      },
      "source": [
        "# Be sure to install the latest version of RLlib.\n",
        "! pip install -U ray[rllib]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ray[rllib]\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/c7/3fb709223d1eae040845abb1bc825d76c3f18ade046063382bc6ace603ef/ray-0.8.7-cp36-cp36m-manylinux1_x86_64.whl (22.0MB)\n",
            "\u001b[K     |████████████████████████████████| 22.0MB 1.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: jsonschema in /usr/local/lib/python3.6/dist-packages (from ray[rllib]) (2.6.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.16 in /usr/local/lib/python3.6/dist-packages (from ray[rllib]) (1.18.5)\n",
            "Collecting aiohttp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/39/7eb5f98d24904e0f6d3edb505d4aa60e3ef83c0a58d6fe18244a51757247/aiohttp-3.6.2-cp36-cp36m-manylinux1_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 43.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from ray[rllib]) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: py-spy>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from ray[rllib]) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from ray[rllib]) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from ray[rllib]) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.28.1 in /usr/local/lib/python3.6/dist-packages (from ray[rllib]) (1.32.0)\n",
            "Collecting colorful\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/8e/e386e248266952d24d73ed734c2f5513f34d9557032618c8910e605dfaf6/colorful-0.5.4-py2.py3-none-any.whl (201kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 45.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: colorama in /usr/local/lib/python3.6/dist-packages (from ray[rllib]) (0.4.3)\n",
            "Collecting aioredis\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/64/1b1612d0a104f21f80eb4c6e1b6075f2e6aba8e228f46f229cfd3fdac859/aioredis-1.3.1-py3-none-any.whl (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from ray[rllib]) (3.13)\n",
            "Collecting gpustat\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/69/d8c849715171aeabd61af7da080fdc60948b5a396d2422f1f4672e43d008/gpustat-0.6.0.tar.gz (78kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.3MB/s \n",
            "\u001b[?25hCollecting opencensus\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/9c/d40e3408e72d02612acf247d829e3fa9ff15c59f7ad81418ed79962f8681/opencensus-0.7.10-py2.py3-none-any.whl (126kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 41.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: google in /usr/local/lib/python3.6/dist-packages (from ray[rllib]) (2.0.3)\n",
            "Requirement already satisfied, skipping upgrade: prometheus-client>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from ray[rllib]) (0.8.0)\n",
            "Collecting redis<3.5.0,>=3.3.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/05/1fc7feedc19c123e7a95cfc9e7892eb6cdd2e5df4e9e8af6384349c1cc3d/redis-3.4.1-py2.py3-none-any.whl (71kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from ray[rllib]) (3.0.12)\n",
            "Requirement already satisfied, skipping upgrade: click>=7.0 in /usr/local/lib/python3.6/dist-packages (from ray[rllib]) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: pandas; extra == \"rllib\" in /usr/local/lib/python3.6/dist-packages (from ray[rllib]) (1.0.5)\n",
            "Requirement already satisfied, skipping upgrade: atari-py; extra == \"rllib\" in /usr/local/lib/python3.6/dist-packages (from ray[rllib]) (0.2.6)\n",
            "Requirement already satisfied, skipping upgrade: tabulate; extra == \"rllib\" in /usr/local/lib/python3.6/dist-packages (from ray[rllib]) (0.8.7)\n",
            "Collecting tensorboardX; extra == \"rllib\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 41.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: gym[atari]; extra == \"rllib\" in /usr/local/lib/python3.6/dist-packages (from ray[rllib]) (0.17.2)\n",
            "Requirement already satisfied, skipping upgrade: dm-tree; extra == \"rllib\" in /usr/local/lib/python3.6/dist-packages (from ray[rllib]) (0.1.5)\n",
            "Collecting lz4; extra == \"rllib\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/6a/ea95dd9a9957143636cfad5037637abec91016b9bde519d3edf4708e3d83/lz4-3.1.0-cp36-cp36m-manylinux2010_x86_64.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 37.8MB/s \n",
            "\u001b[?25hCollecting opencv-python-headless<=4.3.0.36; extra == \"rllib\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/17/e4/a98a3c3098ea55b6ae193a1cd19a221dc3c1bde87a36db5550addc879d36/opencv_python_headless-4.3.0.36-cp36-cp36m-manylinux2014_x86_64.whl (36.4MB)\n",
            "\u001b[K     |████████████████████████████████| 36.4MB 121kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy; extra == \"rllib\" in /usr/local/lib/python3.6/dist-packages (from ray[rllib]) (1.4.1)\n",
            "Collecting async-timeout<4.0,>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
            "Collecting yarl<2.0,>=1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/b4/2cbeaf2c3ea53865d9613b315fe24e78c66acedb1df7e4be4e064c87203b/yarl-1.5.1-cp36-cp36m-manylinux1_x86_64.whl (257kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 40.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray[rllib]) (20.2.0)\n",
            "Collecting idna-ssl>=1.0; python_version < \"3.7\"\n",
            "  Downloading https://files.pythonhosted.org/packages/46/03/07c4894aae38b0de52b52586b24bf189bb83e4ddabfe2e2c8f2419eec6f4/idna-ssl-1.1.0.tar.gz\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.5; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray[rllib]) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray[rllib]) (3.0.4)\n",
            "Collecting multidict<5.0,>=4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/95/f50352b5366e7d579e8b99631680a9e32e1b22adfa1629a8f23b1d22d5e2/multidict-4.7.6-cp36-cp36m-manylinux1_x86_64.whl (148kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 48.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->ray[rllib]) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->ray[rllib]) (50.3.0)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->ray[rllib]) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->ray[rllib]) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->ray[rllib]) (1.24.3)\n",
            "Collecting hiredis\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/7d/6acf1c8d4f2fb327ff6feec000b4c56a20628fbe966a4c7cd16c0b80343c/hiredis-1.1.0-cp36-cp36m-manylinux2010_x86_64.whl (61kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from gpustat->ray[rllib]) (7.352.0)\n",
            "Requirement already satisfied, skipping upgrade: psutil in /usr/local/lib/python3.6/dist-packages (from gpustat->ray[rllib]) (5.4.8)\n",
            "Collecting blessings>=1.6\n",
            "  Downloading https://files.pythonhosted.org/packages/03/74/489f85a78247609c6b4f13733cbf3ba0d864b11aa565617b645d6fdf2a4a/blessings-1.7-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: google-api-core<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from opencensus->ray[rllib]) (1.16.0)\n",
            "Collecting opencensus-context==0.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/2b/b7/720d4507e97aa3916ac47054cd75490de6b6148c46d8c2c487638f16ad95/opencensus_context-0.1.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from google->ray[rllib]) (4.6.3)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas; extra == \"rllib\"->ray[rllib]) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas; extra == \"rllib\"->ray[rllib]) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]; extra == \"rllib\"->ray[rllib]) (1.5.0)\n",
            "Requirement already satisfied, skipping upgrade: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]; extra == \"rllib\"->ray[rllib]) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]; extra == \"rllib\"->ray[rllib]) (7.0.0)\n",
            "Requirement already satisfied, skipping upgrade: opencv-python; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]; extra == \"rllib\"->ray[rllib]) (4.1.2.30)\n",
            "Requirement already satisfied, skipping upgrade: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray[rllib]) (1.52.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2.0dev,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray[rllib]) (1.17.2)\n",
            "Collecting contextvars; python_version >= \"3.6\" and python_version < \"3.7\"\n",
            "  Downloading https://files.pythonhosted.org/packages/83/96/55b82d9f13763be9d672622e1b8106c85acb83edd7cc2fa5bc67cd9877e9/contextvars-2.4.tar.gz\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]; extra == \"rllib\"->ray[rllib]) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0,>=1.0.0->opencensus->ray[rllib]) (4.6)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0,>=1.0.0->opencensus->ray[rllib]) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0,>=1.0.0->opencensus->ray[rllib]) (4.1.1)\n",
            "Collecting immutables>=0.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/e0/ea6fd4697120327d26773b5a84853f897a68e33d3f9376b00a8ff96e4f63/immutables-0.14-cp36-cp36m-manylinux1_x86_64.whl (98kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 10.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0,>=1.0.0->opencensus->ray[rllib]) (0.4.8)\n",
            "Building wheels for collected packages: gpustat, idna-ssl, contextvars\n",
            "  Building wheel for gpustat (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpustat: filename=gpustat-0.6.0-cp36-none-any.whl size=12622 sha256=dca315e338a01a815f5e591501690021332cd7c22fa7cb74f2e932b3fb1caad6\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/b4/d5/fb5b7f1d040f2ff20687e3bad6867d63155dbde5a7c10f4293\n",
            "  Building wheel for idna-ssl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for idna-ssl: filename=idna_ssl-1.1.0-cp36-none-any.whl size=3161 sha256=d213b6ee682c5e7f83875ccd88909340ab14570a7d1231044b880c7899668b04\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/00/b3/32d613e19e08a739751dd6bf998cfed277728f8b2127ad4eb7\n",
            "  Building wheel for contextvars (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for contextvars: filename=contextvars-2.4-cp36-none-any.whl size=7666 sha256=d1618a9dbb8e619b6e7a4188db72dd5229102e72c1278e16d1c9224a763463cc\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/7d/68/1ebae2668bda2228686e3c1cf16f2c2384cea6e9334ad5f6de\n",
            "Successfully built gpustat idna-ssl contextvars\n",
            "Installing collected packages: async-timeout, multidict, yarl, idna-ssl, aiohttp, colorful, hiredis, aioredis, blessings, gpustat, immutables, contextvars, opencensus-context, opencensus, redis, tensorboardX, lz4, opencv-python-headless, ray\n",
            "  Found existing installation: redis 3.5.3\n",
            "    Uninstalling redis-3.5.3:\n",
            "      Successfully uninstalled redis-3.5.3\n",
            "  Found existing installation: ray 0.7.5\n",
            "    Uninstalling ray-0.7.5:\n",
            "      Successfully uninstalled ray-0.7.5\n",
            "Successfully installed aiohttp-3.6.2 aioredis-1.3.1 async-timeout-3.0.1 blessings-1.7 colorful-0.5.4 contextvars-2.4 gpustat-0.6.0 hiredis-1.1.0 idna-ssl-1.1.0 immutables-0.14 lz4-3.1.0 multidict-4.7.6 opencensus-0.7.10 opencensus-context-0.1.1 opencv-python-headless-4.3.0.36 ray-0.8.7 redis-3.4.1 tensorboardX-2.1 yarl-1.5.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cv2"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XwPnR2ibNlh2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "7710e4ca-8414-449b-8e48-283f0e0f09d0"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import gym\n",
        "import ray\n",
        "from ray.rllib.agents.ppo import PPOTrainer, DEFAULT_CONFIG\n",
        "from ray.tune.logger import pretty_print"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tQFzEX2BNlh3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "d3978999-0ace-4939-aa02-236ffd6befd1"
      },
      "source": [
        "# Start up Ray. This must be done before we instantiate any RL agents.\n",
        "ray.init(num_cpus=3, ignore_reinit_error=True, log_to_driver=False)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-09-20 14:55:20,686\tINFO resource_spec.py:231 -- Starting Ray with 7.18 GiB memory available for workers and up to 3.6 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
            "2020-09-20 14:55:21,298\tINFO services.py:1193 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'node_ip_address': '172.28.0.2',\n",
              " 'object_store_address': '/tmp/ray/session_2020-09-20_14-55-20_682981_198/sockets/plasma_store',\n",
              " 'raylet_ip_address': '172.28.0.2',\n",
              " 'raylet_socket_name': '/tmp/ray/session_2020-09-20_14-55-20_682981_198/sockets/raylet',\n",
              " 'redis_address': '172.28.0.2:6379',\n",
              " 'session_dir': '/tmp/ray/session_2020-09-20_14-55-20_682981_198',\n",
              " 'webui_url': 'localhost:8265'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "f9yhpJZVNlh5"
      },
      "source": [
        "Instantiate a PPOTrainer object. We pass in a config object that specifies how the network and training procedure should be configured. Some of the parameters are the following.\n",
        "\n",
        "- `num_workers` is the number of actors that the agent will create. This determines the degree of parallelism that will be used.\n",
        "- `num_sgd_iter` is the number of epochs of SGD (passes through the data) that will be used to optimize the PPO surrogate objective at each iteration of PPO.\n",
        "- `sgd_minibatch_size` is the SGD batch size that will be used to optimize the PPO surrogate objective.\n",
        "- `model` contains a dictionary of parameters describing the neural net used to parameterize the policy. The `fcnet_hiddens` parameter is a list of the sizes of the hidden layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ok210MCfNlh5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "303ed715-16a2-44af-a403-6e021224afad"
      },
      "source": [
        "config = DEFAULT_CONFIG.copy()\n",
        "config['num_workers'] = 1\n",
        "config['num_sgd_iter'] = 30\n",
        "config['sgd_minibatch_size'] = 128\n",
        "config['model']['fcnet_hiddens'] = [100, 100]\n",
        "config['num_cpus_per_worker'] = 0  # This avoids running out of resources in the notebook environment when this cell is re-executed\n",
        "\n",
        "agent = PPOTrainer(config, 'CartPole-v0')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-09-20 14:58:49,787\tINFO trainer.py:605 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
            "2020-09-20 14:58:49,788\tINFO trainer.py:632 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
            "2020-09-20 14:58:54,048\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ty1a6AWVNlh7"
      },
      "source": [
        "Train the policy on the `CartPole-v0` environment for 2 steps. The CartPole problem is described at https://gym.openai.com/envs/CartPole-v0.\n",
        "\n",
        "**EXERCISE:** Inspect how well the policy is doing by looking for the lines that say something like\n",
        "\n",
        "```\n",
        "episode_len_mean: 22.262569832402235\n",
        "episode_reward_mean: 22.262569832402235\n",
        "```\n",
        "\n",
        "This indicates how much reward the policy is receiving and how many time steps of the environment the policy ran. The maximum possible reward for this problem is 200. The reward and trajectory length are very close because the agent receives a reward of one for every time step that it survives (however, that is specific to this environment)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3o0wjdZ3Nlh7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e82032cf-ec3d-4940-d955-98cdc20afa4c"
      },
      "source": [
        "for i in range(2):\n",
        "    result = agent.train()\n",
        "    print(pretty_print(result))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ray/rllib/policy/tf_policy.py:871: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
            "custom_metrics: {}\n",
            "date: 2020-09-20_15-01-31\n",
            "done: false\n",
            "episode_len_mean: 22.3463687150838\n",
            "episode_reward_max: 74.0\n",
            "episode_reward_mean: 22.3463687150838\n",
            "episode_reward_min: 9.0\n",
            "episodes_this_iter: 179\n",
            "episodes_total: 179\n",
            "experiment_id: d93644d90ec74e568ce162f9c39f0b9d\n",
            "hostname: 0ad9898bdec1\n",
            "info:\n",
            "  learner:\n",
            "    default_policy:\n",
            "      cur_kl_coeff: 0.20000000298023224\n",
            "      cur_lr: 4.999999873689376e-05\n",
            "      entropy: 0.6653199791908264\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.028455017134547234\n",
            "      model: {}\n",
            "      policy_loss: -0.034914445132017136\n",
            "      total_loss: 160.1027069091797\n",
            "      vf_explained_var: 0.030042273923754692\n",
            "      vf_loss: 160.13192749023438\n",
            "  num_steps_sampled: 4000\n",
            "  num_steps_trained: 4000\n",
            "iterations_since_restore: 1\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 1\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 9.617333333333331\n",
            "  ram_util_percent: 12.096888888888886\n",
            "pid: 198\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.06908316875630337\n",
            "  mean_inference_ms: 0.7818378647754444\n",
            "  mean_processing_ms: 0.1298263948340918\n",
            "time_since_restore: 7.329290151596069\n",
            "time_this_iter_s: 7.329290151596069\n",
            "time_total_s: 7.329290151596069\n",
            "timers:\n",
            "  learn_throughput: 1240.0\n",
            "  learn_time_ms: 3225.805\n",
            "  load_throughput: 150776.619\n",
            "  load_time_ms: 26.529\n",
            "  sample_throughput: 1001.91\n",
            "  sample_time_ms: 3992.373\n",
            "  update_time_ms: 3.184\n",
            "timestamp: 1600614091\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 4000\n",
            "training_iteration: 1\n",
            "\n",
            "custom_metrics: {}\n",
            "date: 2020-09-20_15-01-38\n",
            "done: false\n",
            "episode_len_mean: 40.68\n",
            "episode_reward_max: 146.0\n",
            "episode_reward_mean: 40.68\n",
            "episode_reward_min: 9.0\n",
            "episodes_this_iter: 94\n",
            "episodes_total: 273\n",
            "experiment_id: d93644d90ec74e568ce162f9c39f0b9d\n",
            "hostname: 0ad9898bdec1\n",
            "info:\n",
            "  learner:\n",
            "    default_policy:\n",
            "      cur_kl_coeff: 0.30000001192092896\n",
            "      cur_lr: 4.999999873689376e-05\n",
            "      entropy: 0.6253743171691895\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.015418531373143196\n",
            "      model: {}\n",
            "      policy_loss: -0.029100561514496803\n",
            "      total_loss: 296.7686462402344\n",
            "      vf_explained_var: 0.020169544965028763\n",
            "      vf_loss: 296.79315185546875\n",
            "  num_steps_sampled: 8000\n",
            "  num_steps_trained: 8000\n",
            "iterations_since_restore: 2\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 1\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 71.91\n",
            "  ram_util_percent: 12.2\n",
            "pid: 198\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.06928427861107651\n",
            "  mean_inference_ms: 0.7814205970920001\n",
            "  mean_processing_ms: 0.12642126318538505\n",
            "time_since_restore: 14.115124940872192\n",
            "time_this_iter_s: 6.785834789276123\n",
            "time_total_s: 14.115124940872192\n",
            "timers:\n",
            "  learn_throughput: 1323.091\n",
            "  learn_time_ms: 3023.223\n",
            "  load_throughput: 284443.962\n",
            "  load_time_ms: 14.063\n",
            "  sample_throughput: 1007.007\n",
            "  sample_time_ms: 3972.167\n",
            "  update_time_ms: 2.857\n",
            "timestamp: 1600614098\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 8000\n",
            "training_iteration: 2\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FPdkWLrENlh9"
      },
      "source": [
        "**EXERCISE:** The current network and training configuration are too large and heavy-duty for a simple problem like CartPole. Modify the configuration to use a smaller network and to speed up the optimization of the surrogate objective (fewer SGD iterations and a larger batch size should help)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3lp6tqkNNlh9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e0580b08-d508-4ee3-eef8-9793b90ef87e"
      },
      "source": [
        "config = DEFAULT_CONFIG.copy()\n",
        "config['num_workers'] = 3\n",
        "config['num_sgd_iter'] = 30\n",
        "config['sgd_minibatch_size'] = 128\n",
        "config['model']['fcnet_hiddens'] = [5, 5]\n",
        "config['num_cpus_per_worker'] = 0\n",
        "\n",
        "agent = PPOTrainer(config, 'CartPole-v0')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-09-20 15:04:17,130\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "64FmVP7kNlh_"
      },
      "source": [
        "**EXERCISE:** Train the agent and try to get a reward of 200. If it's training too slowly you may need to modify the config above to use fewer hidden units, a larger `sgd_minibatch_size`, a smaller `num_sgd_iter`, or a larger `num_workers`.\n",
        "\n",
        "This should take around 20 or 30 training iterations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XB7sdKUzNliA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "08b98dd7-8dc7-4b71-aef2-551bebc3095d"
      },
      "source": [
        "for i in range(20):\n",
        "    result = agent.train()\n",
        "    print(pretty_print(result))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "custom_metrics: {}\n",
            "date: 2020-09-20_15-04-38\n",
            "done: false\n",
            "episode_len_mean: 21.769633507853403\n",
            "episode_reward_max: 92.0\n",
            "episode_reward_mean: 21.769633507853403\n",
            "episode_reward_min: 8.0\n",
            "episodes_this_iter: 191\n",
            "episodes_total: 191\n",
            "experiment_id: af11a82271d24286a20b9d5716351545\n",
            "hostname: 0ad9898bdec1\n",
            "info:\n",
            "  learner:\n",
            "    default_policy:\n",
            "      cur_kl_coeff: 0.20000000298023224\n",
            "      cur_lr: 4.999999873689376e-05\n",
            "      entropy: 0.691760778427124\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.0013493357691913843\n",
            "      model: {}\n",
            "      policy_loss: -0.007289385888725519\n",
            "      total_loss: 222.0408172607422\n",
            "      vf_explained_var: -0.0003922153264284134\n",
            "      vf_loss: 222.04783630371094\n",
            "  num_steps_sampled: 4200\n",
            "  num_steps_trained: 4200\n",
            "iterations_since_restore: 1\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 3\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 54.779999999999994\n",
            "  ram_util_percent: 17.546666666666667\n",
            "pid: 198\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.10243124905739029\n",
            "  mean_inference_ms: 1.704245078810982\n",
            "  mean_processing_ms: 0.2087552613806212\n",
            "time_since_restore: 5.0017900466918945\n",
            "time_this_iter_s: 5.0017900466918945\n",
            "time_total_s: 5.0017900466918945\n",
            "timers:\n",
            "  learn_throughput: 2359.497\n",
            "  learn_time_ms: 1780.041\n",
            "  load_throughput: 165879.553\n",
            "  load_time_ms: 25.32\n",
            "  sample_throughput: 1358.037\n",
            "  sample_time_ms: 3092.7\n",
            "  update_time_ms: 21.306\n",
            "timestamp: 1600614278\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 4200\n",
            "training_iteration: 1\n",
            "\n",
            "custom_metrics: {}\n",
            "date: 2020-09-20_15-04-42\n",
            "done: false\n",
            "episode_len_mean: 23.38888888888889\n",
            "episode_reward_max: 62.0\n",
            "episode_reward_mean: 23.38888888888889\n",
            "episode_reward_min: 9.0\n",
            "episodes_this_iter: 180\n",
            "episodes_total: 371\n",
            "experiment_id: af11a82271d24286a20b9d5716351545\n",
            "hostname: 0ad9898bdec1\n",
            "info:\n",
            "  learner:\n",
            "    default_policy:\n",
            "      cur_kl_coeff: 0.10000000149011612\n",
            "      cur_lr: 4.999999873689376e-05\n",
            "      entropy: 0.6816821694374084\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.005508505739271641\n",
            "      model: {}\n",
            "      policy_loss: -0.014962580986320972\n",
            "      total_loss: 235.3638916015625\n",
            "      vf_explained_var: 0.002593347802758217\n",
            "      vf_loss: 235.3782958984375\n",
            "  num_steps_sampled: 8400\n",
            "  num_steps_trained: 8400\n",
            "iterations_since_restore: 2\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 3\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 90.67142857142856\n",
            "  ram_util_percent: 16.27142857142857\n",
            "pid: 198\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.10138989970270657\n",
            "  mean_inference_ms: 1.6455544469811356\n",
            "  mean_processing_ms: 0.20249786510873827\n",
            "time_since_restore: 9.635584831237793\n",
            "time_this_iter_s: 4.633794784545898\n",
            "time_total_s: 9.635584831237793\n",
            "timers:\n",
            "  learn_throughput: 2406.873\n",
            "  learn_time_ms: 1745.003\n",
            "  load_throughput: 312899.347\n",
            "  load_time_ms: 13.423\n",
            "  sample_throughput: 1401.574\n",
            "  sample_time_ms: 2996.63\n",
            "  update_time_ms: 15.362\n",
            "timestamp: 1600614282\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 8400\n",
            "training_iteration: 2\n",
            "\n",
            "custom_metrics: {}\n",
            "date: 2020-09-20_15-04-47\n",
            "done: false\n",
            "episode_len_mean: 31.23134328358209\n",
            "episode_reward_max: 110.0\n",
            "episode_reward_mean: 31.23134328358209\n",
            "episode_reward_min: 10.0\n",
            "episodes_this_iter: 134\n",
            "episodes_total: 505\n",
            "experiment_id: af11a82271d24286a20b9d5716351545\n",
            "hostname: 0ad9898bdec1\n",
            "info:\n",
            "  learner:\n",
            "    default_policy:\n",
            "      cur_kl_coeff: 0.10000000149011612\n",
            "      cur_lr: 4.999999873689376e-05\n",
            "      entropy: 0.6646074056625366\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.005110759288072586\n",
            "      model: {}\n",
            "      policy_loss: -0.007475768681615591\n",
            "      total_loss: 409.91259765625\n",
            "      vf_explained_var: 0.0007263366132974625\n",
            "      vf_loss: 409.9195861816406\n",
            "  num_steps_sampled: 12600\n",
            "  num_steps_trained: 12600\n",
            "iterations_since_restore: 3\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 3\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 89.11428571428571\n",
            "  ram_util_percent: 16.3\n",
            "pid: 198\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.1003256566384733\n",
            "  mean_inference_ms: 1.6305715107927357\n",
            "  mean_processing_ms: 0.1978351186992687\n",
            "time_since_restore: 14.288758039474487\n",
            "time_this_iter_s: 4.653173208236694\n",
            "time_total_s: 14.288758039474487\n",
            "timers:\n",
            "  learn_throughput: 2411.468\n",
            "  learn_time_ms: 1741.678\n",
            "  load_throughput: 444719.404\n",
            "  load_time_ms: 9.444\n",
            "  sample_throughput: 1417.348\n",
            "  sample_time_ms: 2963.28\n",
            "  update_time_ms: 11.703\n",
            "timestamp: 1600614287\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 12600\n",
            "training_iteration: 3\n",
            "\n",
            "custom_metrics: {}\n",
            "date: 2020-09-20_15-04-52\n",
            "done: false\n",
            "episode_len_mean: 36.578947368421055\n",
            "episode_reward_max: 87.0\n",
            "episode_reward_mean: 36.578947368421055\n",
            "episode_reward_min: 10.0\n",
            "episodes_this_iter: 114\n",
            "episodes_total: 619\n",
            "experiment_id: af11a82271d24286a20b9d5716351545\n",
            "hostname: 0ad9898bdec1\n",
            "info:\n",
            "  learner:\n",
            "    default_policy:\n",
            "      cur_kl_coeff: 0.10000000149011612\n",
            "      cur_lr: 4.999999873689376e-05\n",
            "      entropy: 0.6429598331451416\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.0054525346495211124\n",
            "      model: {}\n",
            "      policy_loss: -0.012622803449630737\n",
            "      total_loss: 456.1453857421875\n",
            "      vf_explained_var: 0.0026012565940618515\n",
            "      vf_loss: 456.157470703125\n",
            "  num_steps_sampled: 16800\n",
            "  num_steps_trained: 16800\n",
            "iterations_since_restore: 4\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 3\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 91.06666666666668\n",
            "  ram_util_percent: 16.3\n",
            "pid: 198\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.10034990737385902\n",
            "  mean_inference_ms: 1.618456689215074\n",
            "  mean_processing_ms: 0.19438128030065235\n",
            "time_since_restore: 18.967217206954956\n",
            "time_this_iter_s: 4.678459167480469\n",
            "time_total_s: 18.967217206954956\n",
            "timers:\n",
            "  learn_throughput: 2404.217\n",
            "  learn_time_ms: 1746.93\n",
            "  load_throughput: 557559.006\n",
            "  load_time_ms: 7.533\n",
            "  sample_throughput: 1426.561\n",
            "  sample_time_ms: 2944.144\n",
            "  update_time_ms: 12.488\n",
            "timestamp: 1600614292\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 16800\n",
            "training_iteration: 4\n",
            "\n",
            "custom_metrics: {}\n",
            "date: 2020-09-20_15-04-56\n",
            "done: false\n",
            "episode_len_mean: 42.22\n",
            "episode_reward_max: 155.0\n",
            "episode_reward_mean: 42.22\n",
            "episode_reward_min: 10.0\n",
            "episodes_this_iter: 96\n",
            "episodes_total: 715\n",
            "experiment_id: af11a82271d24286a20b9d5716351545\n",
            "hostname: 0ad9898bdec1\n",
            "info:\n",
            "  learner:\n",
            "    default_policy:\n",
            "      cur_kl_coeff: 0.10000000149011612\n",
            "      cur_lr: 4.999999873689376e-05\n",
            "      entropy: 0.6241986155509949\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.004292581230401993\n",
            "      model: {}\n",
            "      policy_loss: -0.008242501877248287\n",
            "      total_loss: 643.0054321289062\n",
            "      vf_explained_var: 0.0016029439866542816\n",
            "      vf_loss: 643.0132446289062\n",
            "  num_steps_sampled: 21000\n",
            "  num_steps_trained: 21000\n",
            "iterations_since_restore: 5\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 3\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 90.24285714285713\n",
            "  ram_util_percent: 16.3\n",
            "pid: 198\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.10098680773686385\n",
            "  mean_inference_ms: 1.6355993629261243\n",
            "  mean_processing_ms: 0.19518183875104497\n",
            "time_since_restore: 23.732353687286377\n",
            "time_this_iter_s: 4.765136480331421\n",
            "time_total_s: 23.732353687286377\n",
            "timers:\n",
            "  learn_throughput: 2410.768\n",
            "  learn_time_ms: 1742.184\n",
            "  load_throughput: 660738.331\n",
            "  load_time_ms: 6.357\n",
            "  sample_throughput: 1419.018\n",
            "  sample_time_ms: 2959.793\n",
            "  update_time_ms: 10.813\n",
            "timestamp: 1600614296\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 21000\n",
            "training_iteration: 5\n",
            "\n",
            "custom_metrics: {}\n",
            "date: 2020-09-20_15-05-01\n",
            "done: false\n",
            "episode_len_mean: 50.95\n",
            "episode_reward_max: 153.0\n",
            "episode_reward_mean: 50.95\n",
            "episode_reward_min: 13.0\n",
            "episodes_this_iter: 85\n",
            "episodes_total: 800\n",
            "experiment_id: af11a82271d24286a20b9d5716351545\n",
            "hostname: 0ad9898bdec1\n",
            "info:\n",
            "  learner:\n",
            "    default_policy:\n",
            "      cur_kl_coeff: 0.05000000074505806\n",
            "      cur_lr: 4.999999873689376e-05\n",
            "      entropy: 0.6054508686065674\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.0034616796765476465\n",
            "      model: {}\n",
            "      policy_loss: 0.0012749058660119772\n",
            "      total_loss: 592.38232421875\n",
            "      vf_explained_var: 0.001252962276339531\n",
            "      vf_loss: 592.3809204101562\n",
            "  num_steps_sampled: 25200\n",
            "  num_steps_trained: 25200\n",
            "iterations_since_restore: 6\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 3\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 88.84285714285714\n",
            "  ram_util_percent: 16.3\n",
            "pid: 198\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.10147272549884821\n",
            "  mean_inference_ms: 1.6267813844859038\n",
            "  mean_processing_ms: 0.1922546513844282\n",
            "time_since_restore: 28.388826370239258\n",
            "time_this_iter_s: 4.656472682952881\n",
            "time_total_s: 28.388826370239258\n",
            "timers:\n",
            "  learn_throughput: 2408.055\n",
            "  learn_time_ms: 1744.146\n",
            "  load_throughput: 754602.809\n",
            "  load_time_ms: 5.566\n",
            "  sample_throughput: 1425.443\n",
            "  sample_time_ms: 2946.452\n",
            "  update_time_ms: 10.588\n",
            "timestamp: 1600614301\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 25200\n",
            "training_iteration: 6\n",
            "\n",
            "custom_metrics: {}\n",
            "date: 2020-09-20_15-05-06\n",
            "done: false\n",
            "episode_len_mean: 49.22\n",
            "episode_reward_max: 119.0\n",
            "episode_reward_mean: 49.22\n",
            "episode_reward_min: 10.0\n",
            "episodes_this_iter: 84\n",
            "episodes_total: 884\n",
            "experiment_id: af11a82271d24286a20b9d5716351545\n",
            "hostname: 0ad9898bdec1\n",
            "info:\n",
            "  learner:\n",
            "    default_policy:\n",
            "      cur_kl_coeff: 0.02500000037252903\n",
            "      cur_lr: 4.999999873689376e-05\n",
            "      entropy: 0.5778540372848511\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.0038206095341593027\n",
            "      model: {}\n",
            "      policy_loss: -0.0067298524081707\n",
            "      total_loss: 625.6557006835938\n",
            "      vf_explained_var: 0.0010776054114103317\n",
            "      vf_loss: 625.662353515625\n",
            "  num_steps_sampled: 29400\n",
            "  num_steps_trained: 29400\n",
            "iterations_since_restore: 7\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 3\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 90.58571428571429\n",
            "  ram_util_percent: 16.3\n",
            "pid: 198\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.10201688325074693\n",
            "  mean_inference_ms: 1.636850737414494\n",
            "  mean_processing_ms: 0.19254098134104936\n",
            "time_since_restore: 33.24101543426514\n",
            "time_this_iter_s: 4.852189064025879\n",
            "time_total_s: 33.24101543426514\n",
            "timers:\n",
            "  learn_throughput: 2403.299\n",
            "  learn_time_ms: 1747.597\n",
            "  load_throughput: 842511.684\n",
            "  load_time_ms: 4.985\n",
            "  sample_throughput: 1417.408\n",
            "  sample_time_ms: 2963.155\n",
            "  update_time_ms: 9.628\n",
            "timestamp: 1600614306\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 29400\n",
            "training_iteration: 7\n",
            "\n",
            "custom_metrics: {}\n",
            "date: 2020-09-20_15-05-11\n",
            "done: false\n",
            "episode_len_mean: 59.18\n",
            "episode_reward_max: 154.0\n",
            "episode_reward_mean: 59.18\n",
            "episode_reward_min: 10.0\n",
            "episodes_this_iter: 65\n",
            "episodes_total: 949\n",
            "experiment_id: af11a82271d24286a20b9d5716351545\n",
            "hostname: 0ad9898bdec1\n",
            "info:\n",
            "  learner:\n",
            "    default_policy:\n",
            "      cur_kl_coeff: 0.012500000186264515\n",
            "      cur_lr: 4.999999873689376e-05\n",
            "      entropy: 0.5883668661117554\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.0009668286656960845\n",
            "      model: {}\n",
            "      policy_loss: -0.0026859811041504145\n",
            "      total_loss: 851.7654418945312\n",
            "      vf_explained_var: 0.0007485337555408478\n",
            "      vf_loss: 851.7681274414062\n",
            "  num_steps_sampled: 33600\n",
            "  num_steps_trained: 33600\n",
            "iterations_since_restore: 8\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 3\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 89.0\n",
            "  ram_util_percent: 16.3\n",
            "pid: 198\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.10170291253617435\n",
            "  mean_inference_ms: 1.631802300222195\n",
            "  mean_processing_ms: 0.19112808509636336\n",
            "time_since_restore: 37.85801029205322\n",
            "time_this_iter_s: 4.616994857788086\n",
            "time_total_s: 37.85801029205322\n",
            "timers:\n",
            "  learn_throughput: 2407.189\n",
            "  learn_time_ms: 1744.774\n",
            "  load_throughput: 913057.6\n",
            "  load_time_ms: 4.6\n",
            "  sample_throughput: 1422.892\n",
            "  sample_time_ms: 2951.734\n",
            "  update_time_ms: 8.938\n",
            "timestamp: 1600614311\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 33600\n",
            "training_iteration: 8\n",
            "\n",
            "custom_metrics: {}\n",
            "date: 2020-09-20_15-05-16\n",
            "done: false\n",
            "episode_len_mean: 68.57\n",
            "episode_reward_max: 155.0\n",
            "episode_reward_mean: 68.57\n",
            "episode_reward_min: 19.0\n",
            "episodes_this_iter: 61\n",
            "episodes_total: 1010\n",
            "experiment_id: af11a82271d24286a20b9d5716351545\n",
            "hostname: 0ad9898bdec1\n",
            "info:\n",
            "  learner:\n",
            "    default_policy:\n",
            "      cur_kl_coeff: 0.0062500000931322575\n",
            "      cur_lr: 4.999999873689376e-05\n",
            "      entropy: 0.5834226608276367\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.000862203654833138\n",
            "      model: {}\n",
            "      policy_loss: -0.0015880083665251732\n",
            "      total_loss: 913.3701171875\n",
            "      vf_explained_var: 0.0006584431976079941\n",
            "      vf_loss: 913.3717041015625\n",
            "  num_steps_sampled: 37800\n",
            "  num_steps_trained: 37800\n",
            "iterations_since_restore: 9\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 3\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 90.97142857142856\n",
            "  ram_util_percent: 16.3\n",
            "pid: 198\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.102008973686569\n",
            "  mean_inference_ms: 1.6377310403237206\n",
            "  mean_processing_ms: 0.18978889310550218\n",
            "time_since_restore: 42.70711541175842\n",
            "time_this_iter_s: 4.8491051197052\n",
            "time_total_s: 42.70711541175842\n",
            "timers:\n",
            "  learn_throughput: 2408.372\n",
            "  learn_time_ms: 1743.917\n",
            "  load_throughput: 981287.701\n",
            "  load_time_ms: 4.28\n",
            "  sample_throughput: 1415.51\n",
            "  sample_time_ms: 2967.129\n",
            "  update_time_ms: 8.707\n",
            "timestamp: 1600614316\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 37800\n",
            "training_iteration: 9\n",
            "\n",
            "custom_metrics: {}\n",
            "date: 2020-09-20_15-05-20\n",
            "done: false\n",
            "episode_len_mean: 63.62\n",
            "episode_reward_max: 155.0\n",
            "episode_reward_mean: 63.62\n",
            "episode_reward_min: 15.0\n",
            "episodes_this_iter: 68\n",
            "episodes_total: 1078\n",
            "experiment_id: af11a82271d24286a20b9d5716351545\n",
            "hostname: 0ad9898bdec1\n",
            "info:\n",
            "  learner:\n",
            "    default_policy:\n",
            "      cur_kl_coeff: 0.0031250000465661287\n",
            "      cur_lr: 4.999999873689376e-05\n",
            "      entropy: 0.5488757491111755\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.004604484885931015\n",
            "      model: {}\n",
            "      policy_loss: -0.008043508045375347\n",
            "      total_loss: 752.8201293945312\n",
            "      vf_explained_var: 0.0005603302270174026\n",
            "      vf_loss: 752.828125\n",
            "  num_steps_sampled: 42000\n",
            "  num_steps_trained: 42000\n",
            "iterations_since_restore: 10\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 3\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 91.14999999999999\n",
            "  ram_util_percent: 16.3\n",
            "pid: 198\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.10226036660060131\n",
            "  mean_inference_ms: 1.635069066474615\n",
            "  mean_processing_ms: 0.1891241664254016\n",
            "time_since_restore: 47.28598761558533\n",
            "time_this_iter_s: 4.578872203826904\n",
            "time_total_s: 47.28598761558533\n",
            "timers:\n",
            "  learn_throughput: 2417.793\n",
            "  learn_time_ms: 1737.121\n",
            "  load_throughput: 1048507.348\n",
            "  load_time_ms: 4.006\n",
            "  sample_throughput: 1419.582\n",
            "  sample_time_ms: 2958.617\n",
            "  update_time_ms: 8.646\n",
            "timestamp: 1600614320\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 42000\n",
            "training_iteration: 10\n",
            "\n",
            "custom_metrics: {}\n",
            "date: 2020-09-20_15-05-25\n",
            "done: false\n",
            "episode_len_mean: 71.85\n",
            "episode_reward_max: 158.0\n",
            "episode_reward_mean: 71.85\n",
            "episode_reward_min: 15.0\n",
            "episodes_this_iter: 49\n",
            "episodes_total: 1127\n",
            "experiment_id: af11a82271d24286a20b9d5716351545\n",
            "hostname: 0ad9898bdec1\n",
            "info:\n",
            "  learner:\n",
            "    default_policy:\n",
            "      cur_kl_coeff: 0.0015625000232830644\n",
            "      cur_lr: 4.999999873689376e-05\n",
            "      entropy: 0.5544992089271545\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.004936954006552696\n",
            "      model: {}\n",
            "      policy_loss: -0.0038555944338440895\n",
            "      total_loss: 1096.788818359375\n",
            "      vf_explained_var: 0.0007341727614402771\n",
            "      vf_loss: 1096.792724609375\n",
            "  num_steps_sampled: 46200\n",
            "  num_steps_trained: 46200\n",
            "iterations_since_restore: 11\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 3\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 90.62857142857142\n",
            "  ram_util_percent: 16.3\n",
            "pid: 198\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.10185120705821202\n",
            "  mean_inference_ms: 1.6414254578517122\n",
            "  mean_processing_ms: 0.18838259215715147\n",
            "time_since_restore: 52.05381107330322\n",
            "time_this_iter_s: 4.7678234577178955\n",
            "time_total_s: 52.05381107330322\n",
            "timers:\n",
            "  learn_throughput: 2424.872\n",
            "  learn_time_ms: 1732.05\n",
            "  load_throughput: 2564464.618\n",
            "  load_time_ms: 1.638\n",
            "  sample_throughput: 1423.152\n",
            "  sample_time_ms: 2951.196\n",
            "  update_time_ms: 6.926\n",
            "timestamp: 1600614325\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 46200\n",
            "training_iteration: 11\n",
            "\n",
            "custom_metrics: {}\n",
            "date: 2020-09-20_15-05-30\n",
            "done: false\n",
            "episode_len_mean: 89.19\n",
            "episode_reward_max: 200.0\n",
            "episode_reward_mean: 89.19\n",
            "episode_reward_min: 31.0\n",
            "episodes_this_iter: 42\n",
            "episodes_total: 1169\n",
            "experiment_id: af11a82271d24286a20b9d5716351545\n",
            "hostname: 0ad9898bdec1\n",
            "info:\n",
            "  learner:\n",
            "    default_policy:\n",
            "      cur_kl_coeff: 0.0007812500116415322\n",
            "      cur_lr: 4.999999873689376e-05\n",
            "      entropy: 0.5680054426193237\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.0037138243205845356\n",
            "      model: {}\n",
            "      policy_loss: -0.006014329381287098\n",
            "      total_loss: 1381.1171875\n",
            "      vf_explained_var: 0.0003114808350801468\n",
            "      vf_loss: 1381.123291015625\n",
            "  num_steps_sampled: 50400\n",
            "  num_steps_trained: 50400\n",
            "iterations_since_restore: 12\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 3\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 91.23333333333333\n",
            "  ram_util_percent: 16.3\n",
            "pid: 198\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.10183695567379784\n",
            "  mean_inference_ms: 1.6398085243876448\n",
            "  mean_processing_ms: 0.18800831174952293\n",
            "time_since_restore: 56.609062910079956\n",
            "time_this_iter_s: 4.555251836776733\n",
            "time_total_s: 56.609062910079956\n",
            "timers:\n",
            "  learn_throughput: 2428.92\n",
            "  learn_time_ms: 1729.163\n",
            "  load_throughput: 2526290.574\n",
            "  load_time_ms: 1.663\n",
            "  sample_throughput: 1425.402\n",
            "  sample_time_ms: 2946.537\n",
            "  update_time_ms: 6.341\n",
            "timestamp: 1600614330\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 50400\n",
            "training_iteration: 12\n",
            "\n",
            "custom_metrics: {}\n",
            "date: 2020-09-20_15-05-34\n",
            "done: false\n",
            "episode_len_mean: 98.61\n",
            "episode_reward_max: 200.0\n",
            "episode_reward_mean: 98.61\n",
            "episode_reward_min: 32.0\n",
            "episodes_this_iter: 42\n",
            "episodes_total: 1211\n",
            "experiment_id: af11a82271d24286a20b9d5716351545\n",
            "hostname: 0ad9898bdec1\n",
            "info:\n",
            "  learner:\n",
            "    default_policy:\n",
            "      cur_kl_coeff: 0.0003906250058207661\n",
            "      cur_lr: 4.999999873689376e-05\n",
            "      entropy: 0.5489048361778259\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.005161839071661234\n",
            "      model: {}\n",
            "      policy_loss: -0.003538419958204031\n",
            "      total_loss: 1287.666259765625\n",
            "      vf_explained_var: 0.0003051590174436569\n",
            "      vf_loss: 1287.669677734375\n",
            "  num_steps_sampled: 54600\n",
            "  num_steps_trained: 54600\n",
            "iterations_since_restore: 13\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 3\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 89.88571428571427\n",
            "  ram_util_percent: 16.3\n",
            "pid: 198\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.10170862169130541\n",
            "  mean_inference_ms: 1.6381642568596144\n",
            "  mean_processing_ms: 0.1873321448563734\n",
            "time_since_restore: 61.24087357521057\n",
            "time_this_iter_s: 4.631810665130615\n",
            "time_total_s: 61.24087357521057\n",
            "timers:\n",
            "  learn_throughput: 2454.683\n",
            "  learn_time_ms: 1711.015\n",
            "  load_throughput: 2523323.278\n",
            "  load_time_ms: 1.664\n",
            "  sample_throughput: 1417.564\n",
            "  sample_time_ms: 2962.829\n",
            "  update_time_ms: 6.235\n",
            "timestamp: 1600614334\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 54600\n",
            "training_iteration: 13\n",
            "\n",
            "custom_metrics: {}\n",
            "date: 2020-09-20_15-05-39\n",
            "done: false\n",
            "episode_len_mean: 108.6\n",
            "episode_reward_max: 200.0\n",
            "episode_reward_mean: 108.6\n",
            "episode_reward_min: 32.0\n",
            "episodes_this_iter: 32\n",
            "episodes_total: 1243\n",
            "experiment_id: af11a82271d24286a20b9d5716351545\n",
            "hostname: 0ad9898bdec1\n",
            "info:\n",
            "  learner:\n",
            "    default_policy:\n",
            "      cur_kl_coeff: 0.0003906250058207661\n",
            "      cur_lr: 4.999999873689376e-05\n",
            "      entropy: 0.551245927810669\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.001114176819100976\n",
            "      model: {}\n",
            "      policy_loss: -0.0013941656798124313\n",
            "      total_loss: 1625.978515625\n",
            "      vf_explained_var: 0.00014557689428329468\n",
            "      vf_loss: 1625.9798583984375\n",
            "  num_steps_sampled: 58800\n",
            "  num_steps_trained: 58800\n",
            "iterations_since_restore: 14\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 3\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 89.15714285714286\n",
            "  ram_util_percent: 16.3\n",
            "pid: 198\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.10161820816522844\n",
            "  mean_inference_ms: 1.6413815001348604\n",
            "  mean_processing_ms: 0.18719907379182046\n",
            "time_since_restore: 65.87061524391174\n",
            "time_this_iter_s: 4.629741668701172\n",
            "time_total_s: 65.87061524391174\n",
            "timers:\n",
            "  learn_throughput: 2462.004\n",
            "  learn_time_ms: 1705.928\n",
            "  load_throughput: 2566594.324\n",
            "  load_time_ms: 1.636\n",
            "  sample_throughput: 1416.952\n",
            "  sample_time_ms: 2964.11\n",
            "  update_time_ms: 5.146\n",
            "timestamp: 1600614339\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 58800\n",
            "training_iteration: 14\n",
            "\n",
            "custom_metrics: {}\n",
            "date: 2020-09-20_15-05-43\n",
            "done: false\n",
            "episode_len_mean: 116.81\n",
            "episode_reward_max: 200.0\n",
            "episode_reward_mean: 116.81\n",
            "episode_reward_min: 32.0\n",
            "episodes_this_iter: 31\n",
            "episodes_total: 1274\n",
            "experiment_id: af11a82271d24286a20b9d5716351545\n",
            "hostname: 0ad9898bdec1\n",
            "info:\n",
            "  learner:\n",
            "    default_policy:\n",
            "      cur_kl_coeff: 0.00019531250291038305\n",
            "      cur_lr: 4.999999873689376e-05\n",
            "      entropy: 0.5525490641593933\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.0014859368093311787\n",
            "      model: {}\n",
            "      policy_loss: 0.00335843488574028\n",
            "      total_loss: 1590.7861328125\n",
            "      vf_explained_var: 9.705312550067902e-05\n",
            "      vf_loss: 1590.78271484375\n",
            "  num_steps_sampled: 63000\n",
            "  num_steps_trained: 63000\n",
            "iterations_since_restore: 15\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 3\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 93.60000000000001\n",
            "  ram_util_percent: 16.3\n",
            "pid: 198\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.10164668523833352\n",
            "  mean_inference_ms: 1.6417426991976811\n",
            "  mean_processing_ms: 0.1873095733566987\n",
            "time_since_restore: 70.51182126998901\n",
            "time_this_iter_s: 4.6412060260772705\n",
            "time_total_s: 70.51182126998901\n",
            "timers:\n",
            "  learn_throughput: 2465.511\n",
            "  learn_time_ms: 1703.501\n",
            "  load_throughput: 2462409.393\n",
            "  load_time_ms: 1.706\n",
            "  sample_throughput: 1421.789\n",
            "  sample_time_ms: 2954.024\n",
            "  update_time_ms: 5.076\n",
            "timestamp: 1600614343\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 63000\n",
            "training_iteration: 15\n",
            "\n",
            "custom_metrics: {}\n",
            "date: 2020-09-20_15-05-48\n",
            "done: false\n",
            "episode_len_mean: 133.3\n",
            "episode_reward_max: 200.0\n",
            "episode_reward_mean: 133.3\n",
            "episode_reward_min: 57.0\n",
            "episodes_this_iter: 29\n",
            "episodes_total: 1303\n",
            "experiment_id: af11a82271d24286a20b9d5716351545\n",
            "hostname: 0ad9898bdec1\n",
            "info:\n",
            "  learner:\n",
            "    default_policy:\n",
            "      cur_kl_coeff: 9.765625145519152e-05\n",
            "      cur_lr: 4.999999873689376e-05\n",
            "      entropy: 0.5411416888237\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.0008281051414087415\n",
            "      model: {}\n",
            "      policy_loss: 4.089903086423874e-05\n",
            "      total_loss: 1742.2208251953125\n",
            "      vf_explained_var: 5.061738193035126e-05\n",
            "      vf_loss: 1742.220703125\n",
            "  num_steps_sampled: 67200\n",
            "  num_steps_trained: 67200\n",
            "iterations_since_restore: 16\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 3\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 89.42857142857143\n",
            "  ram_util_percent: 16.3\n",
            "pid: 198\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.10169416909297552\n",
            "  mean_inference_ms: 1.6398125476202807\n",
            "  mean_processing_ms: 0.18668003108078224\n",
            "time_since_restore: 75.11546444892883\n",
            "time_this_iter_s: 4.603643178939819\n",
            "time_total_s: 75.11546444892883\n",
            "timers:\n",
            "  learn_throughput: 2473.071\n",
            "  learn_time_ms: 1698.293\n",
            "  load_throughput: 2453936.897\n",
            "  load_time_ms: 1.712\n",
            "  sample_throughput: 1421.932\n",
            "  sample_time_ms: 2953.728\n",
            "  update_time_ms: 4.464\n",
            "timestamp: 1600614348\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 67200\n",
            "training_iteration: 16\n",
            "\n",
            "custom_metrics: {}\n",
            "date: 2020-09-20_15-05-53\n",
            "done: false\n",
            "episode_len_mean: 136.76\n",
            "episode_reward_max: 200.0\n",
            "episode_reward_mean: 136.76\n",
            "episode_reward_min: 58.0\n",
            "episodes_this_iter: 32\n",
            "episodes_total: 1335\n",
            "experiment_id: af11a82271d24286a20b9d5716351545\n",
            "hostname: 0ad9898bdec1\n",
            "info:\n",
            "  learner:\n",
            "    default_policy:\n",
            "      cur_kl_coeff: 4.882812572759576e-05\n",
            "      cur_lr: 4.999999873689376e-05\n",
            "      entropy: 0.539114236831665\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.0008719385368749499\n",
            "      model: {}\n",
            "      policy_loss: 0.0010597456712275743\n",
            "      total_loss: 1471.795166015625\n",
            "      vf_explained_var: 2.969801425933838e-05\n",
            "      vf_loss: 1471.7940673828125\n",
            "  num_steps_sampled: 71400\n",
            "  num_steps_trained: 71400\n",
            "iterations_since_restore: 17\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 3\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 91.9857142857143\n",
            "  ram_util_percent: 16.3\n",
            "pid: 198\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.10166764747925824\n",
            "  mean_inference_ms: 1.6377089742380706\n",
            "  mean_processing_ms: 0.18605378435080966\n",
            "time_since_restore: 79.74498748779297\n",
            "time_this_iter_s: 4.629523038864136\n",
            "time_total_s: 79.74498748779297\n",
            "timers:\n",
            "  learn_throughput: 2484.003\n",
            "  learn_time_ms: 1690.819\n",
            "  load_throughput: 1933155.938\n",
            "  load_time_ms: 2.173\n",
            "  sample_throughput: 1429.322\n",
            "  sample_time_ms: 2938.457\n",
            "  update_time_ms: 4.57\n",
            "timestamp: 1600614353\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 71400\n",
            "training_iteration: 17\n",
            "\n",
            "custom_metrics: {}\n",
            "date: 2020-09-20_15-05-57\n",
            "done: false\n",
            "episode_len_mean: 136.56\n",
            "episode_reward_max: 200.0\n",
            "episode_reward_mean: 136.56\n",
            "episode_reward_min: 58.0\n",
            "episodes_this_iter: 32\n",
            "episodes_total: 1367\n",
            "experiment_id: af11a82271d24286a20b9d5716351545\n",
            "hostname: 0ad9898bdec1\n",
            "info:\n",
            "  learner:\n",
            "    default_policy:\n",
            "      cur_kl_coeff: 2.441406286379788e-05\n",
            "      cur_lr: 4.999999873689376e-05\n",
            "      entropy: 0.5283502340316772\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.0017084116116166115\n",
            "      model: {}\n",
            "      policy_loss: 0.0012240423820912838\n",
            "      total_loss: 1441.6700439453125\n",
            "      vf_explained_var: 1.9101426005363464e-05\n",
            "      vf_loss: 1441.6688232421875\n",
            "  num_steps_sampled: 75600\n",
            "  num_steps_trained: 75600\n",
            "iterations_since_restore: 18\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 3\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 91.28333333333335\n",
            "  ram_util_percent: 16.3\n",
            "pid: 198\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.10162664585567818\n",
            "  mean_inference_ms: 1.636175726216888\n",
            "  mean_processing_ms: 0.18568772767678607\n",
            "time_since_restore: 84.39353895187378\n",
            "time_this_iter_s: 4.6485514640808105\n",
            "time_total_s: 84.39353895187378\n",
            "timers:\n",
            "  learn_throughput: 2489.634\n",
            "  learn_time_ms: 1686.995\n",
            "  load_throughput: 1950277.528\n",
            "  load_time_ms: 2.154\n",
            "  sample_throughput: 1425.863\n",
            "  sample_time_ms: 2945.584\n",
            "  update_time_ms: 4.575\n",
            "timestamp: 1600614357\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 75600\n",
            "training_iteration: 18\n",
            "\n",
            "custom_metrics: {}\n",
            "date: 2020-09-20_15-06-02\n",
            "done: false\n",
            "episode_len_mean: 133.66\n",
            "episode_reward_max: 200.0\n",
            "episode_reward_mean: 133.66\n",
            "episode_reward_min: 58.0\n",
            "episodes_this_iter: 32\n",
            "episodes_total: 1399\n",
            "experiment_id: af11a82271d24286a20b9d5716351545\n",
            "hostname: 0ad9898bdec1\n",
            "info:\n",
            "  learner:\n",
            "    default_policy:\n",
            "      cur_kl_coeff: 1.220703143189894e-05\n",
            "      cur_lr: 4.999999873689376e-05\n",
            "      entropy: 0.5303695201873779\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.0016974018653854728\n",
            "      model: {}\n",
            "      policy_loss: -0.003961533308029175\n",
            "      total_loss: 1596.7294921875\n",
            "      vf_explained_var: 1.2014061212539673e-05\n",
            "      vf_loss: 1596.7335205078125\n",
            "  num_steps_sampled: 79800\n",
            "  num_steps_trained: 79800\n",
            "iterations_since_restore: 19\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 3\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 90.12857142857145\n",
            "  ram_util_percent: 16.3\n",
            "pid: 198\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.10164194043343684\n",
            "  mean_inference_ms: 1.634743212558881\n",
            "  mean_processing_ms: 0.18540850481983517\n",
            "time_since_restore: 89.04556226730347\n",
            "time_this_iter_s: 4.6520233154296875\n",
            "time_total_s: 89.04556226730347\n",
            "timers:\n",
            "  learn_throughput: 2487.676\n",
            "  learn_time_ms: 1688.322\n",
            "  load_throughput: 1965400.007\n",
            "  load_time_ms: 2.137\n",
            "  sample_throughput: 1436.074\n",
            "  sample_time_ms: 2924.64\n",
            "  update_time_ms: 4.76\n",
            "timestamp: 1600614362\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 79800\n",
            "training_iteration: 19\n",
            "\n",
            "custom_metrics: {}\n",
            "date: 2020-09-20_15-06-07\n",
            "done: false\n",
            "episode_len_mean: 137.57\n",
            "episode_reward_max: 200.0\n",
            "episode_reward_mean: 137.57\n",
            "episode_reward_min: 58.0\n",
            "episodes_this_iter: 28\n",
            "episodes_total: 1427\n",
            "experiment_id: af11a82271d24286a20b9d5716351545\n",
            "hostname: 0ad9898bdec1\n",
            "info:\n",
            "  learner:\n",
            "    default_policy:\n",
            "      cur_kl_coeff: 6.10351571594947e-06\n",
            "      cur_lr: 4.999999873689376e-05\n",
            "      entropy: 0.5272420644760132\n",
            "      entropy_coeff: 0.0\n",
            "      kl: 0.0006276833591982722\n",
            "      model: {}\n",
            "      policy_loss: 0.0004522395320236683\n",
            "      total_loss: 1513.831298828125\n",
            "      vf_explained_var: 1.1175870895385742e-05\n",
            "      vf_loss: 1513.830810546875\n",
            "  num_steps_sampled: 84000\n",
            "  num_steps_trained: 84000\n",
            "iterations_since_restore: 20\n",
            "node_ip: 172.28.0.2\n",
            "num_healthy_workers: 3\n",
            "off_policy_estimator: {}\n",
            "perf:\n",
            "  cpu_util_percent: 90.82857142857142\n",
            "  ram_util_percent: 16.3\n",
            "pid: 198\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_env_wait_ms: 0.10161658203327933\n",
            "  mean_inference_ms: 1.6321186870825335\n",
            "  mean_processing_ms: 0.18500338023533108\n",
            "time_since_restore: 93.69954514503479\n",
            "time_this_iter_s: 4.653982877731323\n",
            "time_total_s: 93.69954514503479\n",
            "timers:\n",
            "  learn_throughput: 2475.511\n",
            "  learn_time_ms: 1696.619\n",
            "  load_throughput: 1952612.206\n",
            "  load_time_ms: 2.151\n",
            "  sample_throughput: 1436.36\n",
            "  sample_time_ms: 2924.059\n",
            "  update_time_ms: 4.278\n",
            "timestamp: 1600614367\n",
            "timesteps_since_restore: 0\n",
            "timesteps_total: 84000\n",
            "training_iteration: 20\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PW6bN9CYNliB"
      },
      "source": [
        "Checkpoint the current model. The call to `agent.save()` returns the path to the checkpointed model and can be used later to restore the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6uf808LMNliC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f7ae09e0-585b-4bf1-a424-be54f7ea807a"
      },
      "source": [
        "checkpoint_path = agent.save()\n",
        "print(checkpoint_path)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/ray_results/PPO_CartPole-v0_2020-09-20_15-04-09yncubysb/checkpoint_20/checkpoint-20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "05icI8bfNliD"
      },
      "source": [
        "Now let's use the trained policy to make predictions.\n",
        "\n",
        "**NOTE:** Here we are loading the trained policy in the same process, but in practice, this would often be done in a different process (probably on a different machine)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8Qq2_AYVNliE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "402203a3-7e63-4dbf-c943-c5641eb7ac11"
      },
      "source": [
        "trained_config = config.copy()\n",
        "\n",
        "test_agent = PPOTrainer(trained_config, 'CartPole-v0')\n",
        "test_agent.restore(checkpoint_path)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-09-20 15:06:55,875\tWARNING worker.py:1134 -- WARNING: 9 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
            "2020-09-20 15:06:58,199\tWARNING worker.py:1134 -- WARNING: 10 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
            "2020-09-20 15:07:03,219\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n",
            "2020-09-20 15:07:03,574\tINFO trainable.py:473 -- Restored on 172.28.0.2 from checkpoint: /root/ray_results/PPO_CartPole-v0_2020-09-20_15-04-09yncubysb/checkpoint_20/checkpoint-20\n",
            "2020-09-20 15:07:03,578\tINFO trainable.py:480 -- Current state after restoring: {'_iteration': 20, '_timesteps_total': None, '_time_total': 93.69954514503479, '_episodes_total': 1427}\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "c2gUUlqkNliG"
      },
      "source": [
        "Now use the trained policy to act in an environment. The key line is the call to `test_agent.compute_action(state)` which uses the trained policy to choose an action.\n",
        "\n",
        "**EXERCISE:** Verify that the reward received roughly matches up with the reward printed in the training logs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9asL5Z5lNliH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7bc0bc4f-358b-41c5-9798-5b3b6a16e7a0"
      },
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "state = env.reset()\n",
        "done = False\n",
        "cumulative_reward = 0\n",
        "\n",
        "while not done:\n",
        "    action = test_agent.compute_action(state)\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    cumulative_reward += reward\n",
        "\n",
        "print(cumulative_reward)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "162.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iB1ORu-yNliJ"
      },
      "source": [
        "# RLlib Exercise 2 - Custom Environments and Reward Shaping\n",
        "\n",
        "**GOAL:** The goal of this exercise is to demonstrate how to adapt your own problem to use RLlib.\n",
        "\n",
        "To understand how to use **RLlib**, see the documentation at http://rllib.io.\n",
        "\n",
        "RLlib is not only easy to use in simulated benchmarks but also in the real-world. Here, we will cover two important concepts: how to create your own Markov Decision Process abstraction, and how to shape the reward of your environment so make your agent more effective. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtrkS5afELDR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ray.shutdown()"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Csnr34cnNliK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "bacdf362-ae32-4f35-9c96-7e1b830199ab"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "from tutorial.rllib_exercises import test_exercises\n",
        "\n",
        "import ray\n",
        "from ray.rllib.agents.ppo import PPOTrainer, DEFAULT_CONFIG\n",
        "\n",
        "ray.init(ignore_reinit_error=True, log_to_driver=False)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-09-20 15:10:18,597\tINFO resource_spec.py:231 -- Starting Ray with 7.13 GiB memory available for workers and up to 3.58 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
            "2020-09-20 15:10:19,073\tINFO services.py:1193 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'node_ip_address': '172.28.0.2',\n",
              " 'object_store_address': '/tmp/ray/session_2020-09-20_15-10-18_591511_198/sockets/plasma_store',\n",
              " 'raylet_ip_address': '172.28.0.2',\n",
              " 'raylet_socket_name': '/tmp/ray/session_2020-09-20_15-10-18_591511_198/sockets/raylet',\n",
              " 'redis_address': '172.28.0.2:6379',\n",
              " 'session_dir': '/tmp/ray/session_2020-09-20_15-10-18_591511_198',\n",
              " 'webui_url': 'localhost:8265'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Wg0Xb06MNliM"
      },
      "source": [
        "## 1. Different Spaces\n",
        "\n",
        "The first thing to do when formulating an RL problem is to specify the dimensions of your observation space and action space. Abstractions for these are provided in ``gym``. \n",
        "\n",
        "### **Exercise 1:** Match different actions to their corresponding space.\n",
        "\n",
        "The purpose of this exercise is to familiarize you with different Gym spaces. For example:\n",
        "\n",
        "    discrete = spaces.Discrete(10)\n",
        "    print(\"Random sample of this space: \", [discrete.sample() for i in range(4)])\n",
        "\n",
        "Use `help(spaces)` or `help([specific space])` (i.e., `help(spaces.Discrete)`) for more info."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fx9pGt_wEh3P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "help(spaces)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7RfODlKgNliM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "57aa5015-4564-4a96-aaae-6af0f58ea2c7"
      },
      "source": [
        "action_space_map = {\n",
        "    \"discrete_10\": spaces.Discrete(10),\n",
        "    \"box_1\": spaces.Box(0, 1, shape=(1,)),\n",
        "    \"box_3x1\": spaces.Box(-2, 2, shape=(3, 1)),\n",
        "    \"multi_discrete\": spaces.MultiDiscrete([ 5, 2, 2, 4 ])\n",
        "}\n",
        "\n",
        "action_space_jumble = {\n",
        "    \"discrete_10\": 1,\n",
        "    \"multi_discrete\": np.array([0, 0, 0, 2]),\n",
        "    \"box_3x1\": np.array([[-1.2657754], [-1.6528835], [ 0.5982418]]),\n",
        "    \"box_1\": np.array([0.89089584]),\n",
        "}\n",
        "\n",
        "\n",
        "for space_id, state in action_space_jumble.items():\n",
        "    assert action_space_map[space_id].contains(state), (\n",
        "        \"Looks like {} to {} is matched incorrectly.\".format(space_id, state))\n",
        "    \n",
        "print(\"Success!\")"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "l9rrsSmsNliO"
      },
      "source": [
        "## **Exercise 2**: Setting up a custom environment with rewards\n",
        "\n",
        "We'll setup an `n-Chain` environment, which presents moves along a linear chain of states, with two actions:\n",
        "\n",
        "     (0) forward, which moves along the chain but returns no reward\n",
        "     (1) backward, which returns to the beginning and has a small reward\n",
        "\n",
        "The end of the chain, however, presents a large reward, and by moving 'forward', at the end of the chain this large reward can be repeated.\n",
        "\n",
        "#### Step 1: Implement ``ChainEnv._setup_spaces``\n",
        "\n",
        "We'll use a `spaces.Discrete` action space and observation space. Implement `ChainEnv._setup_spaces` so that `self.action_space` and `self.obseration_space` are proper gym spaces.\n",
        "  \n",
        "1. Observation space is an integer in ``[0 to n-1]``.\n",
        "2. Action space is an integer in ``[0, 1]``.\n",
        "\n",
        "For example:\n",
        "\n",
        "```python\n",
        "    self.action_space = spaces.Discrete(2)\n",
        "    self.observation_space = ...\n",
        "```\n",
        "\n",
        "You should see a message indicating tests passing when done correctly!\n",
        "\n",
        "#### Step 2: Implement a reward function.\n",
        "\n",
        "When `env.step` is called, it returns a tuple of ``(state, reward, done, info)``. Right now, the reward is always 0. \n",
        "\n",
        "Implement it so that \n",
        "\n",
        "1. ``action == 1`` will return `self.small_reward`.\n",
        "2. ``action == 0`` will return 0 if `self.state < self.n - 1`.\n",
        "3. ``action == 0`` will return `self.large_reward` if `self.state == self.n - 1`.\n",
        "\n",
        "You should see a message indicating tests passing when done correctly. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SFpjKjqTNliO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "dc64413e-892c-4ae7-c5ee-34526d2d028e"
      },
      "source": [
        "class ChainEnv(gym.Env):\n",
        "    \n",
        "    def __init__(self, env_config = None):\n",
        "        env_config = env_config or {}\n",
        "        self.n = env_config.get(\"n\", 20)\n",
        "        self.small_reward = env_config.get(\"small\", 2)  # payout for 'backwards' action\n",
        "        self.large_reward = env_config.get(\"large\", 10)  # payout at end of chain for 'forwards' action\n",
        "        self.state = 0  # Start at beginning of the chain\n",
        "        self._horizon = self.n\n",
        "        self._counter = 0  # For terminating the episode\n",
        "        self._setup_spaces()\n",
        "    \n",
        "    def _setup_spaces(self):\n",
        "        ##############\n",
        "        # TODO: Implement this so that it passes tests\n",
        "        self.action_space = spaces.Discrete(2)\n",
        "        self.observation_space = spaces.Discrete(self.n)\n",
        "        ##############\n",
        "\n",
        "    def step(self, action):\n",
        "        assert self.action_space.contains(action)\n",
        "        if action == 1:  # 'backwards': go back to the beginning, get small reward\n",
        "            ##############|\n",
        "            # TODO 2: Implement this so that it passes tests\n",
        "            reward = self.small_reward\n",
        "            ##############\n",
        "            self.state = 0\n",
        "        elif self.state < self.n - 1:  # 'forwards': go up along the chain\n",
        "            ##############\n",
        "            # TODO 2: Implement this so that it passes tests\n",
        "            reward = 0\n",
        "            self.state += 1\n",
        "        else:  # 'forwards': stay at the end of the chain, collect large reward\n",
        "            ##############\n",
        "            # TODO 2: Implement this so that it passes tests\n",
        "            reward = self.large_reward\n",
        "            ##############\n",
        "        self._counter += 1\n",
        "        done = self._counter >= self._horizon\n",
        "        return self.state, reward, done, {}\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = 0\n",
        "        self._counter = 0\n",
        "        return self.state\n",
        "    \n",
        "# Tests here:\n",
        "test_exercises.test_chain_env_spaces(ChainEnv)\n",
        "test_exercises.test_chain_env_reward(ChainEnv)"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing if spaces have been setup correctly...\n",
            "Success! You've setup the spaces correctly.\n",
            "Testing if reward has been setup correctly...\n",
            "Success! You've setup the rewards correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yqMJtzLNNliQ"
      },
      "source": [
        "### Now let's train a policy on the environment and evaluate this policy on our environment.\n",
        "\n",
        "You'll see that despite an extremely high reward, the policy has barely explored the state space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FyXbhXoANliQ",
        "colab": {}
      },
      "source": [
        "trainer_config = DEFAULT_CONFIG.copy()\n",
        "trainer_config['num_workers'] = 1\n",
        "trainer_config[\"train_batch_size\"] = 400\n",
        "trainer_config[\"sgd_minibatch_size\"] = 64\n",
        "trainer_config[\"num_sgd_iter\"] = 10"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uoNYqFPcNliS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "outputId": "91d4ec64-805c-440c-a192-cf1a7e1a81d6"
      },
      "source": [
        "trainer = PPOTrainer(trainer_config, ChainEnv);\n",
        "for i in range(20):\n",
        "    print(\"Training iteration {}...\".format(i))\n",
        "    trainer.train()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-09-20 15:38:24,048\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training iteration 0...\n",
            "Training iteration 1...\n",
            "Training iteration 2...\n",
            "Training iteration 3...\n",
            "Training iteration 4...\n",
            "Training iteration 5...\n",
            "Training iteration 6...\n",
            "Training iteration 7...\n",
            "Training iteration 8...\n",
            "Training iteration 9...\n",
            "Training iteration 10...\n",
            "Training iteration 11...\n",
            "Training iteration 12...\n",
            "Training iteration 13...\n",
            "Training iteration 14...\n",
            "Training iteration 15...\n",
            "Training iteration 16...\n",
            "Training iteration 17...\n",
            "Training iteration 18...\n",
            "Training iteration 19...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nP6xXGNONliT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "0bbf1e72-2c55-4b02-e084-49fde414cd72"
      },
      "source": [
        "env = ChainEnv({})\n",
        "state = env.reset()\n",
        "\n",
        "done = False\n",
        "max_state = -1\n",
        "cumulative_reward = 0\n",
        "\n",
        "while not done:\n",
        "    action = trainer.compute_action(state)\n",
        "    state, reward, done, results = env.step(action)\n",
        "    max_state = max(max_state, state)\n",
        "    cumulative_reward += reward\n",
        "\n",
        "print(\"Cumulative reward you've received is: {}. Congratulations!\".format(cumulative_reward))\n",
        "print(\"Max state you've visited is: {}. This is out of {} states.\".format(max_state, env.n))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cumulative reward you've received is: 22. Congratulations!\n",
            "Max state you've visited is: 3. This is out of 20 states.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nbdppkbENliV"
      },
      "source": [
        "## Exercise 3: Shaping the reward to encourage proper behavior.\n",
        "\n",
        "You'll see that despite an extremely high reward, the policy has barely explored the state space. This is often the situation - where the reward designed to encourage a particular solution is suboptimal, and the behavior created is unintended.\n",
        "\n",
        "#### Modify `ShapedChainEnv.step` to provide a reward that encourages the policy to traverse the chain (not just stick to 0). Do not change the behavior of the environment (the action -> state behavior should be the same).\n",
        "\n",
        "You can change the reward to be whatever you wish."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PfyPqX1ZNliV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ac732981-1d11-45b8-c541-ab4ac6cca820"
      },
      "source": [
        "class ShapedChainEnv(ChainEnv):\n",
        "    def step(self, action):\n",
        "        assert self.action_space.contains(action)\n",
        "        if action == 1:  # 'backwards': go back to the beginning\n",
        "            reward = -1\n",
        "            self.state = 0\n",
        "        elif self.state < self.n - 1:  # 'forwards': go up along the chain\n",
        "            reward = 10\n",
        "            self.state += 1\n",
        "        else:  # 'forwards': stay at the end of the chain\n",
        "            reward = 1000\n",
        "        self._counter += 1\n",
        "        done = self._counter >= self._horizon\n",
        "        return self.state, reward, done, {}\n",
        "    \n",
        "test_exercises.test_chain_env_behavior(ShapedChainEnv)"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing if behavior has been changed...\n",
            "Success! Behavior of environment is correct.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ty6H4MffNliW"
      },
      "source": [
        "### Evaluate `ShapedChainEnv` by running the cell below.\n",
        "\n",
        "This trains PPO on the new env and counts the number of states seen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8U6GkAO6NliX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "outputId": "ba568bff-d1e1-4cfe-aa1d-9a9bc15de52f"
      },
      "source": [
        "trainer = PPOTrainer(trainer_config, ShapedChainEnv);\n",
        "for i in range(20):\n",
        "    print(\"Training iteration {}...\".format(i))\n",
        "    trainer.train()\n",
        "\n",
        "env = ShapedChainEnv({})\n",
        "\n",
        "max_states = []\n",
        "\n",
        "for i in range(10):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    max_state = -1\n",
        "    cumulative_reward = 0\n",
        "    while not done:\n",
        "        action = trainer.compute_action(state)\n",
        "        state, reward, done, results = env.step(action)\n",
        "        max_state = max(max_state, state)\n",
        "        cumulative_reward += reward\n",
        "    max_states += [max_state]\n",
        "\n",
        "\n",
        "print(max_states)\n",
        "print(\"Cumulative reward you've received is: {}!\".format(cumulative_reward))\n",
        "print(\"Max state you've visited is: {}. This is out of {} states.\".format(np.mean(max_state), env.n))\n",
        "assert (env.n - np.mean(max_state)) / env.n < 0.2, \"This policy did not traverse many states.\""
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-09-20 16:29:58,334\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training iteration 0...\n",
            "Training iteration 1...\n",
            "Training iteration 2...\n",
            "Training iteration 3...\n",
            "Training iteration 4...\n",
            "Training iteration 5...\n",
            "Training iteration 6...\n",
            "Training iteration 7...\n",
            "Training iteration 8...\n",
            "Training iteration 9...\n",
            "Training iteration 10...\n",
            "Training iteration 11...\n",
            "Training iteration 12...\n",
            "Training iteration 13...\n",
            "Training iteration 14...\n",
            "Training iteration 15...\n",
            "Training iteration 16...\n",
            "Training iteration 17...\n",
            "Training iteration 18...\n",
            "Training iteration 19...\n",
            "[3, 7, 3, 5, 3, 3, 4, 2, 5, 4]\n",
            "Cumulative reward you've received is: 123!\n",
            "Max state you've visited is: 4.0. This is out of 20 states.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-136-bf4bc53c3018>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cumulative reward you've received is: {}!\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcumulative_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Max state you've visited is: {}. This is out of {} states.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"This policy did not traverse many states.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m: This policy did not traverse many states."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Sj5ynpINNliZ",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}