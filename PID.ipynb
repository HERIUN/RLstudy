{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gym import Env as GymEnv\n",
    "import gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0002\n",
    "gamma = 0.98\n",
    "class PIDpolicy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PIDpolicy, self).__init__()\n",
    "        self.data = []\n",
    "        \n",
    "        self.fc1 = nn.Linear(4,128)\n",
    "        self.fc2 = nn.Linear(128,3)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def put_data(self, item):\n",
    "        self.data.append(item)\n",
    "    \n",
    "    def train_net(self):\n",
    "        R = 0\n",
    "        self.optimizer.zero_grad()\n",
    "        for r, prob in self.data[::-1]:\n",
    "            R = r+gamma*R\n",
    "            loss = -torch.log(prob)*R\n",
    "            loss.mean().backward()\n",
    "        self.optimizer.step()\n",
    "        self.data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicAgent(object):\n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "    def initialize(self, state):\n",
    "        pass\n",
    "    def pretraining_act(self, state): # training 전 데이터 모으기용\n",
    "        return self.action_space.sample()\n",
    "    def training_acct(self, state): # training용\n",
    "        return self.action_space.sample()\n",
    "    def solving_act(self, state): # test용\n",
    "        return self.action_space.sample()\n",
    "    def pretraining_react(self, state, reward): #tr\n",
    "        pass\n",
    "    def training_react(self, state, reward):\n",
    "        pass\n",
    "    def solving_react(self, state, reward):\n",
    "        pass  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PIDControlAgent(BasicAgent):\n",
    "    def __init__(self, action_space, fs, kp=1.2, ki=1.0, kd=0.001, set_angle=0):\n",
    "        # action_space : gym.spaces : 가능한 액션\n",
    "        # fs : sampling frequency. (Hz) == 50.\n",
    "        # kp : gain of proportional controller\n",
    "        # ki : gain of integral controller\n",
    "        # kd : derivative controller\n",
    "        super(PIDControlAgent, self).__init__(action_space)\n",
    "        self.kp = kp\n",
    "        self.ki = ki\n",
    "        self.kd = kd\n",
    "        \n",
    "        self.set_angle = set_angle #원하는 각도 : 0도가 이상적\n",
    "        self.tau = 1.0/fs\n",
    "        \n",
    "        self.p_term = 0.0\n",
    "        self.i_term = 0.0\n",
    "        self.d_term = 0.0\n",
    "\n",
    "        # cache\n",
    "        self.output = 0.0\n",
    "        self.err_prev = 0.0\n",
    "                \n",
    "    def update(self, v_in, v_fb):\n",
    "        # v_in : input command : 원하는 각도\n",
    "        # v_fb : feedback from observer : 현재 각도\n",
    "        # output : output command??\n",
    "        # u(t) = K_p e(t) + K_i \\int_{0}^{t} e(t)dt + K_d {de}/{dt}\n",
    "        err = v_in - v_fb # 0 - 현재각\n",
    "        \n",
    "        #Ziegler–Nichols method\n",
    "        self.p_term = err\n",
    "        self.i_term += err*self.tau\n",
    "        self.d_term = (err - self.err_prev)*self.tau\n",
    "        self.output = self.kp*self.p_term + self.ki*self.i_term + self.kd*self.d_term\n",
    "        \n",
    "        self.err_prev = err\n",
    "        \n",
    "        return self.output\n",
    "        \n",
    "    def choose_action(self, val):\n",
    "        if val >= 0:\n",
    "            action = 0\n",
    "        else:\n",
    "            action = 1\n",
    "        return action\n",
    "    \n",
    "    def solving_act(self, state):\n",
    "        output = self.update(self.set_angle, state[2])\n",
    "        temp = self.choose_action(output)\n",
    "        self.action = temp\n",
    "        return self.action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicSolver(object):\n",
    "    def __init__(self, env=None, agent=None, policy=None,\n",
    "                 skip_pretraining=False,\n",
    "                 skip_training=False,\n",
    "                 skip_solving=False):\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        self.policy = policy\n",
    "    def pretrain(self):\n",
    "        pass\n",
    "    def train(self):\n",
    "        pass\n",
    "    def solve(self):\n",
    "        pass\n",
    "    def run(self):\n",
    "        self.solve()\n",
    "    def terminate(self):\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleSolver(BasicSolver):\n",
    "    def __init__(self,\n",
    "                 solving_episodes=10,\n",
    "                 max_steps = 200,\n",
    "                 render_when_sovling=True,\n",
    "                 **kwargs):\n",
    "        super(CartPoleSolver, self).__init__(**kwargs)\n",
    "\n",
    "        self.solving_episodes = solving_episodes\n",
    "        self.max_steps = max_steps\n",
    "\n",
    "        # flags control for rendering\n",
    "        self.rws = render_when_sovling\n",
    "\n",
    "    def solve(self):\n",
    "        state = self.env.reset()\n",
    "        self.agent.initialize(state)\n",
    "\n",
    "        for i in range(100):\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "\n",
    "                action = self.agent.solving_act(state)\n",
    "                state, reward, done, info = self.env.step(action)\n",
    "                k = self.policy(torch.from_numpy(state).float())\n",
    "                self.policy.put_data((reward, k))\n",
    "                kp = k[0].detach().numpy()\n",
    "                ki = k[1].detach().numpy()\n",
    "                kd = k[2].detach().numpy()\n",
    "                self.agent.kp = kp\n",
    "                self.agent.ki = ki\n",
    "                self.agent.kd = kd\n",
    "\n",
    "                total_reward += reward\n",
    "\n",
    "                if done:\n",
    "                    self.policy.train_net()\n",
    "                    print(f'Kp:{self.agent.kp}, Ki:{self.agent.ki}, Kd:{self.agent.kd}')\n",
    "                    print('Episode: {}'.format(i),\n",
    "                          'Total reward: {}'.format(total_reward))\n",
    "                    self.env.reset()\n",
    "        self.env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pid_control_solver():\n",
    "    env = gym.make('CartPole-v0')\n",
    "    # NOTE: kp, ki, kd are tuned manually, they are not the optimal parameter\n",
    "    # for this PID controller\n",
    "    kp = np.random.randint(10)\n",
    "    ki = np.random.randint(10)\n",
    "    kp = np.random.randint(10)\n",
    "    print('initial kp,ki,kp', kp,ki,kp)\n",
    "    policy = PIDpolicy()\n",
    "    \n",
    "    agent = PIDControlAgent(env.action_space, \n",
    "                        env.metadata['video.frames_per_second'],\n",
    "                        kp=kp, ki=ki, kd=kp)\n",
    "    # NOTE: pretraining and training stage is not required for this solver\n",
    "    solver = CartPoleSolver(env=env, agent=agent, policy=policy,\n",
    "                            skip_pretraining=True,\n",
    "                            skip_training=True)\n",
    "    solver.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    pid_control_solver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial kp,ki,kp 1 8 1\n",
      "Kp:0.0, Ki:0.0, Kd:0.6038234829902649\n",
      "Episode: 0 Total reward: 200.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.505026638507843\n",
      "Episode: 1 Total reward: 200.0\n",
      "Kp:0.0538405179977417, Ki:0.0, Kd:0.46948158740997314\n",
      "Episode: 2 Total reward: 200.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.7927125096321106\n",
      "Episode: 3 Total reward: 190.0\n",
      "Kp:0.060383278876543045, Ki:0.0, Kd:0.4833754897117615\n",
      "Episode: 4 Total reward: 200.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.3643105626106262\n",
      "Episode: 5 Total reward: 200.0\n",
      "Kp:0.10099588334560394, Ki:0.0, Kd:0.5354748368263245\n",
      "Episode: 6 Total reward: 200.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.7578005194664001\n",
      "Episode: 7 Total reward: 200.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.7709106802940369\n",
      "Episode: 8 Total reward: 200.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.8895102739334106\n",
      "Episode: 9 Total reward: 132.0\n",
      "Kp:0.04670775681734085, Ki:0.0, Kd:0.5311923623085022\n",
      "Episode: 10 Total reward: 200.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.7372403740882874\n",
      "Episode: 11 Total reward: 200.0\n",
      "Kp:0.03812767565250397, Ki:0.0, Kd:0.44594040513038635\n",
      "Episode: 12 Total reward: 200.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.27625253796577454\n",
      "Episode: 13 Total reward: 200.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.9223302602767944\n",
      "Episode: 14 Total reward: 171.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.9197852611541748\n",
      "Episode: 15 Total reward: 169.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.19603244960308075\n",
      "Episode: 16 Total reward: 200.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.34998881816864014\n",
      "Episode: 17 Total reward: 200.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.7067160606384277\n",
      "Episode: 18 Total reward: 200.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.7918512225151062\n",
      "Episode: 19 Total reward: 200.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.8798209428787231\n",
      "Episode: 20 Total reward: 111.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.041019320487976\n",
      "Episode: 21 Total reward: 156.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.8275147676467896\n",
      "Episode: 22 Total reward: 200.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.0188184976577759\n",
      "Episode: 23 Total reward: 148.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.3017379641532898\n",
      "Episode: 24 Total reward: 200.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.31478625535964966\n",
      "Episode: 25 Total reward: 200.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.0774983167648315\n",
      "Episode: 26 Total reward: 143.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.1502211093902588\n",
      "Episode: 27 Total reward: 109.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.1511428356170654\n",
      "Episode: 28 Total reward: 137.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.084100604057312\n",
      "Episode: 29 Total reward: 192.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.2431849241256714\n",
      "Episode: 30 Total reward: 111.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.1119223833084106\n",
      "Episode: 31 Total reward: 122.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.20561945438385\n",
      "Episode: 32 Total reward: 129.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.9237568378448486\n",
      "Episode: 33 Total reward: 200.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.3963415324687958\n",
      "Episode: 34 Total reward: 200.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.9583885073661804\n",
      "Episode: 35 Total reward: 200.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.6522664427757263\n",
      "Episode: 36 Total reward: 200.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.425143301486969\n",
      "Episode: 37 Total reward: 200.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
