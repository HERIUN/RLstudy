{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gym import Env as GymEnv\n",
    "import gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0002\n",
    "gamma = 0.98\n",
    "class PIDpolicy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PIDpolicy, self).__init__()\n",
    "        self.data = []\n",
    "        self.fc1 = nn.Linear(4,128)\n",
    "        self.fc2 = nn.Linear(128,3)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def forward(self, x): # state를 넣었을때 output으로 kp,ki,kd값이 나오도록 함. 이때 각 값은 양수이므로 relu사용\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x\n",
    "    \n",
    "    def put_data(self, item): # reward와 해당 kp,ki,kd값 저장\n",
    "        self.data.append(item)\n",
    "    \n",
    "    def train_net(self):\n",
    "        R = 0\n",
    "        self.optimizer.zero_grad()\n",
    "#         print(self.data)\n",
    "        for r, k in self.data[::-1]:\n",
    "            R = r+gamma*R\n",
    "            loss = -torch.log(k)*R\n",
    "            loss.mean().backward(retain_graph=True)\n",
    "        self.optimizer.step()\n",
    "        self.data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicAgent(object):\n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "    def initialize(self, state):\n",
    "        pass\n",
    "    def pretraining_act(self, state): # training 전 데이터 모으기용\n",
    "        return self.action_space.sample()\n",
    "    def training_acct(self, state): # training용\n",
    "        return self.action_space.sample()\n",
    "    def solving_act(self, state): # test용\n",
    "        return self.action_space.sample()\n",
    "    def pretraining_react(self, state, reward): #tr\n",
    "        pass\n",
    "    def training_react(self, state, reward):\n",
    "        pass\n",
    "    def solving_react(self, state, reward):\n",
    "        pass  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PIDControlAgent(BasicAgent):\n",
    "    def __init__(self, action_space, fs, kp=1.2, ki=1.0, kd=0.001, set_angle=0):\n",
    "        # action_space : gym.spaces : 가능한 액션\n",
    "        # fs : sampling frequency. (Hz) == 50.\n",
    "        # kp : gain of proportional controller\n",
    "        # ki : gain of integral controller\n",
    "        # kd : derivative controller\n",
    "        super(PIDControlAgent, self).__init__(action_space)\n",
    "        self.kp = kp\n",
    "        self.ki = ki\n",
    "        self.kd = kd\n",
    "        \n",
    "        self.set_angle = set_angle #원하는 각도 : 0도가 이상적\n",
    "        self.tau = 1.0/fs\n",
    "        \n",
    "        self.p_term = 0.0\n",
    "        self.i_term = 0.0\n",
    "        self.d_term = 0.0\n",
    "\n",
    "        # cache\n",
    "        self.output = 0.0\n",
    "        self.err_prev = 0.0\n",
    "                \n",
    "    def update(self, v_in, v_fb):\n",
    "        # v_in : input command : 원하는 각도\n",
    "        # v_fb : feedback from observer : 현재 각도\n",
    "        # output : output command??\n",
    "        # u(t) = K_p e(t) + K_i \\int_{0}^{t} e(t)dt + K_d {de}/{dt}\n",
    "        err = v_in - v_fb # 0 - 현재각\n",
    "        \n",
    "        #Ziegler–Nichols method\n",
    "        self.p_term = err\n",
    "        self.i_term += err*self.tau\n",
    "        self.d_term = (err - self.err_prev)*self.tau\n",
    "        self.output = self.kp*self.p_term + self.ki*self.i_term + self.kd*self.d_term\n",
    "        \n",
    "        self.err_prev = err\n",
    "        \n",
    "        return self.output\n",
    "        \n",
    "    def choose_action(self, val):\n",
    "        if val >= 0:\n",
    "            action = 0\n",
    "        else:\n",
    "            action = 1\n",
    "        return action\n",
    "    \n",
    "    def solving_act(self, state):\n",
    "        output = self.update(self.set_angle, state[2])\n",
    "        temp = self.choose_action(output)\n",
    "        self.action = temp\n",
    "        return self.action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicSolver(object):\n",
    "    def __init__(self, env=None, agent=None, policy=None,\n",
    "                 skip_pretraining=False,\n",
    "                 skip_training=False,\n",
    "                 skip_solving=False):\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        self.policy = policy\n",
    "    def pretrain(self):\n",
    "        pass\n",
    "    def train(self):\n",
    "        pass\n",
    "    def solve(self):\n",
    "        pass\n",
    "    def run(self):\n",
    "        self.solve()\n",
    "    def terminate(self):\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleSolver(BasicSolver):\n",
    "    def __init__(self,\n",
    "                 solving_episodes=10,\n",
    "                 max_steps = 200,\n",
    "                 render_when_sovling=True,\n",
    "                 **kwargs):\n",
    "        super(CartPoleSolver, self).__init__(**kwargs)\n",
    "\n",
    "        self.solving_episodes = solving_episodes\n",
    "        self.max_steps = max_steps\n",
    "\n",
    "        # flags control for rendering\n",
    "        self.rws = render_when_sovling\n",
    "\n",
    "    def solve(self):\n",
    "        state = self.env.reset()\n",
    "        self.agent.initialize(state)\n",
    "        k = Variable(torch.from_numpy(np.array([self.agent.kp,self.agent.ki,self.agent.kd])), requires_grad=True)\n",
    "        for i in range(100):\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                action = self.agent.solving_act(state)\n",
    "                state, reward, done, info = self.env.step(action)\n",
    "                total_reward += reward\n",
    "                self.policy.put_data((reward, k))\n",
    "\n",
    "                if done:\n",
    "                    print(f'Kp:{self.agent.kp}, Ki:{self.agent.ki}, Kd:{self.agent.kd}')\n",
    "                    print('Episode: {}'.format(i),\n",
    "                          'Total reward: {}'.format(total_reward))\n",
    "                    self.policy.train_net()\n",
    "                    k = self.policy(torch.from_numpy(state).float())\n",
    "                    kp = k[0].detach().numpy()\n",
    "                    ki = k[1].detach().numpy()\n",
    "                    kd = k[2].detach().numpy()\n",
    "                    self.agent.kp = kp\n",
    "                    self.agent.ki = ki\n",
    "                    self.agent.kd = kd\n",
    "                    self.env.reset()\n",
    "        self.env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pid_control_solver():\n",
    "    env = gym.make('CartPole-v0')\n",
    "    # NOTE: kp, ki, kd are tuned manually, they are not the optimal parameter\n",
    "    # for this PID controller\n",
    "    kp = np.random.randn()\n",
    "    ki = np.random.randn()\n",
    "    kp = np.random.randn()\n",
    "    policy = PIDpolicy()\n",
    "    \n",
    "    agent = PIDControlAgent(env.action_space, \n",
    "                        env.metadata['video.frames_per_second'],\n",
    "                        kp=kp, ki=ki, kd=kp)\n",
    "    # NOTE: pretraining and training stage is not required for this solver\n",
    "    solver = CartPoleSolver(env=env, agent=agent, policy=policy,\n",
    "                            skip_pretraining=True,\n",
    "                            skip_training=True)\n",
    "    solver.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    pid_control_solver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kp:0.5407308250026835, Ki:0.4483093471928245, Kd:0.5407308250026835\n",
      "Episode: 0 Total reward: 41.0\n",
      "Kp:0.3162427842617035, Ki:0.0, Kd:0.4445227384567261\n",
      "Episode: 1 Total reward: 38.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.48022395372390747\n",
      "Episode: 2 Total reward: 182.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.495945543050766\n",
      "Episode: 3 Total reward: 163.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.5376630425453186\n",
      "Episode: 4 Total reward: 125.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.5271545648574829\n",
      "Episode: 5 Total reward: 200.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.8598750829696655\n",
      "Episode: 6 Total reward: 143.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.4965572655200958\n",
      "Episode: 7 Total reward: 180.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.9582056999206543\n",
      "Episode: 8 Total reward: 200.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.5267021059989929\n",
      "Episode: 9 Total reward: 120.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.5868900418281555\n",
      "Episode: 10 Total reward: 171.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.1766180992126465\n",
      "Episode: 11 Total reward: 146.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.1212505102157593\n",
      "Episode: 12 Total reward: 200.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.783280074596405\n",
      "Episode: 13 Total reward: 154.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.6089338064193726\n",
      "Episode: 14 Total reward: 157.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.6297160983085632\n",
      "Episode: 15 Total reward: 151.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.1828210353851318\n",
      "Episode: 16 Total reward: 200.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.5703811049461365\n",
      "Episode: 17 Total reward: 196.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.6300963759422302\n",
      "Episode: 18 Total reward: 177.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.6496227383613586\n",
      "Episode: 19 Total reward: 184.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.131954550743103\n",
      "Episode: 20 Total reward: 200.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.6731845736503601\n",
      "Episode: 21 Total reward: 116.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.7319982051849365\n",
      "Episode: 22 Total reward: 140.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.3044853210449219\n",
      "Episode: 23 Total reward: 184.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.3172636032104492\n",
      "Episode: 24 Total reward: 182.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.634901762008667\n",
      "Episode: 25 Total reward: 200.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.0275686979293823\n",
      "Episode: 26 Total reward: 178.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.6823655962944031\n",
      "Episode: 27 Total reward: 132.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.7922142148017883\n",
      "Episode: 28 Total reward: 200.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.688426673412323\n",
      "Episode: 29 Total reward: 191.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.7900035977363586\n",
      "Episode: 30 Total reward: 200.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.7138910293579102\n",
      "Episode: 31 Total reward: 181.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.8387672901153564\n",
      "Episode: 32 Total reward: 200.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.7319212555885315\n",
      "Episode: 33 Total reward: 190.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.8729072213172913\n",
      "Episode: 34 Total reward: 122.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.8913875222206116\n",
      "Episode: 35 Total reward: 118.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.8426821827888489\n",
      "Episode: 36 Total reward: 200.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.6346465349197388\n",
      "Episode: 37 Total reward: 142.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.912737250328064\n",
      "Episode: 38 Total reward: 148.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.88287752866745\n",
      "Episode: 39 Total reward: 188.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.9087621569633484\n",
      "Episode: 40 Total reward: 133.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.9090856313705444\n",
      "Episode: 41 Total reward: 147.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.470452070236206\n",
      "Episode: 42 Total reward: 171.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.8922195434570312\n",
      "Episode: 43 Total reward: 116.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.9139436483383179\n",
      "Episode: 44 Total reward: 190.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.4481568336486816\n",
      "Episode: 45 Total reward: 200.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.1018661260604858\n",
      "Episode: 46 Total reward: 142.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.3173898458480835\n",
      "Episode: 47 Total reward: 132.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.9694020748138428\n",
      "Episode: 48 Total reward: 175.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.0758103132247925\n",
      "Episode: 49 Total reward: 148.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.8965778946876526\n",
      "Episode: 50 Total reward: 191.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.0975046157836914\n",
      "Episode: 51 Total reward: 131.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.0354599952697754\n",
      "Episode: 52 Total reward: 190.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.4032801389694214\n",
      "Episode: 53 Total reward: 200.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.9504666328430176\n",
      "Episode: 54 Total reward: 106.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.4145586490631104\n",
      "Episode: 55 Total reward: 119.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.5073760747909546\n",
      "Episode: 56 Total reward: 200.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.8897945880889893\n",
      "Episode: 57 Total reward: 131.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.0880370140075684\n",
      "Episode: 58 Total reward: 113.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.9592664837837219\n",
      "Episode: 59 Total reward: 184.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.0859062671661377\n",
      "Episode: 60 Total reward: 200.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.9670528173446655\n",
      "Episode: 61 Total reward: 191.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.3620765209197998\n",
      "Episode: 62 Total reward: 162.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.4089889526367188\n",
      "Episode: 63 Total reward: 190.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.5460174083709717\n",
      "Episode: 64 Total reward: 140.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.4334601163864136\n",
      "Episode: 65 Total reward: 140.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.1252883672714233\n",
      "Episode: 66 Total reward: 200.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.9992413520812988\n",
      "Episode: 67 Total reward: 181.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.5490320920944214\n",
      "Episode: 68 Total reward: 193.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.0969237089157104\n",
      "Episode: 69 Total reward: 151.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.6746755838394165\n",
      "Episode: 70 Total reward: 198.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.6163772344589233\n",
      "Episode: 71 Total reward: 200.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.8998168110847473\n",
      "Episode: 72 Total reward: 141.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.2027899026870728\n",
      "Episode: 73 Total reward: 185.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.4679456949234009\n",
      "Episode: 74 Total reward: 200.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.0796698331832886\n",
      "Episode: 75 Total reward: 193.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.0737496614456177\n",
      "Episode: 76 Total reward: 155.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.161657452583313\n",
      "Episode: 77 Total reward: 200.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.6643463969230652\n",
      "Episode: 78 Total reward: 153.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.1927508115768433\n",
      "Episode: 79 Total reward: 200.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.3316670656204224\n",
      "Episode: 80 Total reward: 134.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.7439249753952026\n",
      "Episode: 81 Total reward: 168.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.6167793273925781\n",
      "Episode: 82 Total reward: 165.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.6920634508132935\n",
      "Episode: 83 Total reward: 185.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.7126108407974243\n",
      "Episode: 84 Total reward: 180.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.1469100713729858\n",
      "Episode: 85 Total reward: 176.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.249415636062622\n",
      "Episode: 86 Total reward: 200.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.5827696323394775\n",
      "Episode: 87 Total reward: 172.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.2704070806503296\n",
      "Episode: 88 Total reward: 200.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.9477754831314087\n",
      "Episode: 89 Total reward: 159.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.5899683237075806\n",
      "Episode: 90 Total reward: 161.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.5403565168380737\n",
      "Episode: 91 Total reward: 200.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.4590171575546265\n",
      "Episode: 92 Total reward: 150.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.7503470182418823\n",
      "Episode: 93 Total reward: 166.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.9332979917526245\n",
      "Episode: 94 Total reward: 174.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.6513229608535767\n",
      "Episode: 95 Total reward: 169.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.6674996614456177\n",
      "Episode: 96 Total reward: 200.0\n",
      "Kp:0.0, Ki:0.0, Kd:0.9041790962219238\n",
      "Episode: 97 Total reward: 174.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.3416926860809326\n",
      "Episode: 98 Total reward: 100.0\n",
      "Kp:0.0, Ki:0.0, Kd:1.189542293548584\n",
      "Episode: 99 Total reward: 181.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
